{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "1KH1kvjFQ5a-"
      ],
      "authorship_tag": "ABX9TyO3jU7k8DvfnLtNoCjxIp2a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaejunchoe/HAIDS-Lab/blob/main/Upload_D_attn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwtITRADQpDw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 준비준비"
      ],
      "metadata": {
        "id": "1KH1kvjFQ5a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1번 실행 -> 런타임 다시 시작 -> 2번 실행 -> 3번 실행\n",
        "\n",
        "## 1번\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q97Yc9hBQ6pc",
        "outputId": "40ff6fd2-7636-467d-d0d1-3f4bc6fe3528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 3s (3,576 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123633 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2번\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic')"
      ],
      "metadata": {
        "id": "iPDuj2z0Q7ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3번\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import seaborn as sns\n",
        "\n",
        "# 나눔고딕 폰트 경로 설정\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
        "plt.rc('font', family=font_name)\n",
        "\n",
        "# 그래프 그리기 예시\n",
        "data = {'가나다': [10, 15, 7], '라마바': [20, 8, 12]}\n",
        "df = pd.DataFrame(data)\n",
        "sns.barplot(data=df, x='가나다', y='라마바')\n",
        "plt.title('한글 그래프')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "prl3g6SAQ8Ws",
        "outputId": "4bac7b38-d6e3-4da2-a752-e032209d7641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시작"
      ],
      "metadata": {
        "id": "IF_xwQWCQ9-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdcftNTBQ_n0",
        "outputId": "1894cb91-af8a-43ef-9a46-51a0be0f84a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data_loader"
      ],
      "metadata": {
        "id": "Fh3J2-gVR08O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from nltk import word_tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, csv_path, emb_path, max_len=10000, embed_size=64):\n",
        "        \"\"\"\n",
        "        데이터셋 클래스\n",
        "        :param csv_path: 전처리된 CSV 파일 경로\n",
        "        :param emb_path: 단어 임베딩 파일 경로\n",
        "        :param max_len: 리뷰 최대 길이\n",
        "        :param embed_size: 임베딩 차원\n",
        "        \"\"\"\n",
        "        self.dataset = pd.read_csv(csv_path, header=None, names=['userID', 'itemID', 'review', 'rating'])\n",
        "\n",
        "        # 단어 임베딩 로드\n",
        "        with open(emb_path, 'rb') as f:\n",
        "            self.word_emb = pickle.load(f)\n",
        "\n",
        "        # 임베딩 벡터 설정\n",
        "        self.pad = np.zeros(embed_size)\n",
        "        self.unknown = np.random.uniform(0, 1, embed_size)\n",
        "        self.delimiter = np.random.uniform(0, 1, embed_size)\n",
        "\n",
        "        # 하이퍼파라미터\n",
        "        self.max_len = max_len\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        하나의 데이터 샘플을 가져옵니다.\n",
        "        \"\"\"\n",
        "        row = self.dataset.loc[index]\n",
        "        user_id = row['userID']\n",
        "        item_id = row['itemID']\n",
        "\n",
        "        # 리뷰 데이터\n",
        "        user_review = self.preprocess_review(user_id, \"User\")\n",
        "        item_review = self.preprocess_review(item_id, \"Item\")\n",
        "\n",
        "        # 평점\n",
        "        rating = torch.tensor(row['rating'], dtype=torch.float)\n",
        "\n",
        "        return torch.tensor(user_review, dtype=torch.float), \\\n",
        "               torch.tensor(item_review, dtype=torch.float), \\\n",
        "               rating\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def preprocess_review(self, entity_id, entity_type):\n",
        "        \"\"\"\n",
        "        리뷰 파일을 로드하고 임베딩을 적용합니다.\n",
        "        \"\"\"\n",
        "        file_path = f\"data/{entity_type}/{entity_id}.tsv\"\n",
        "\n",
        "        try:\n",
        "            reviews = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "        except Exception:\n",
        "            return [self.pad] * self.max_len\n",
        "\n",
        "        total_review = []\n",
        "        for review_str in reviews[0][:100]:  # 최대 100개의 리뷰 단어\n",
        "            tokens = word_tokenize(review_str)\n",
        "            for word in tokens:\n",
        "                if word in self.word_emb:\n",
        "                    total_review.append(self.word_emb[word])\n",
        "                else:\n",
        "                    total_review.append(self.unknown)\n",
        "            total_review.append(self.delimiter)\n",
        "\n",
        "        # 리뷰 길이 조정 (패딩 또는 자르기)\n",
        "        if len(total_review) < self.max_len:\n",
        "            pad_len = self.max_len - len(total_review)\n",
        "            total_review += [self.pad] * pad_len\n",
        "        else:\n",
        "            total_review = total_review[:self.max_len]\n",
        "\n",
        "        return np.array(total_review)\n",
        "\n",
        "\n",
        "def my_collate(batch):\n",
        "    \"\"\"\n",
        "    None 데이터를 필터링하는 Collate 함수.\n",
        "    \"\"\"\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    return default_collate(batch)\n",
        "\n",
        "\n",
        "def get_loader(csv_path, emb_path, batch_size=32, shuffle=True, num_workers=2):\n",
        "    \"\"\"\n",
        "    데이터 로더 생성 함수.\n",
        "    :param csv_path: 전처리된 CSV 파일 경로\n",
        "    :param emb_path: 단어 임베딩 파일 경로\n",
        "    \"\"\"\n",
        "    dataset = ReviewDataset(csv_path, emb_path)\n",
        "    data_loader = DataLoader(dataset=dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=my_collate)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "# 파일 경로 설정\n",
        "train_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2_train.csv'\n",
        "valid_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2__valid.csv'\n",
        "test_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2_test.csv'\n",
        "emb_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/combined_word_emb.pkl'\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_loader = get_loader(train_path, emb_path, batch_size=32, shuffle=True)\n",
        "valid_loader = get_loader(valid_path, emb_path, batch_size=32, shuffle=False)\n",
        "test_loader = get_loader(test_path, emb_path, batch_size=32, shuffle=False)\n",
        "\n",
        "# 데이터 확인\n",
        "for i, (user_review, item_review, rating) in enumerate(train_loader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"User Review Tensor: {user_review.shape}\")\n",
        "    print(f\"Item Review Tensor: {item_review.shape}\")\n",
        "    print(f\"Rating Tensor: {rating.shape}\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "zwBmtLhnR1H1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebca5343-abcc-47b8-da49-265f59274821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "User Review Tensor: torch.Size([32, 10000, 64])\n",
            "Item Review Tensor: torch.Size([32, 10000, 64])\n",
            "Rating Tensor: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 너무 느리다면? 아래의 코드로 수정\n",
        "\n",
        "\n",
        "\n",
        "def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    하나의 데이터 샘플을 가져옵니다.\n",
        "    \"\"\"\n",
        "    row = self.dataset.loc[index]\n",
        "    user_id = row['userID']\n",
        "    item_id = row['itemID']\n",
        "    \n",
        "    # 리뷰 데이터\n",
        "    user_review = np.array(self.preprocess_review(user_id, \"User\"))  # numpy 변환 추가\n",
        "    item_review = np.array(self.preprocess_review(item_id, \"Item\"))  # numpy 변환 추가\n",
        "    \n",
        "    # 평점\n",
        "    rating = torch.tensor(row['rating'], dtype=torch.float)\n",
        "    \n",
        "    return torch.tensor(user_review, dtype=torch.float), \\\n",
        "           torch.tensor(item_review, dtype=torch.float), \\\n",
        "           rating\n"
      ],
      "metadata": {
        "id": "uDY-66P6Wm_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture"
      ],
      "metadata": {
        "id": "55zVZHr7lPN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "bLLoVgw0lcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LocalAttention(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, win_size, out_channels):\n",
        "        super(LocalAttention, self).__init__()\n",
        "\n",
        "        self.win_size = win_size\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 1, kernel_size=(win_size, embed_size)),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(1, embed_size)),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Padding dynamically to ensure compatibility\n",
        "        #padding = torch.zeros(x.size(0), (self.win_size - 1) // 2, x.size(-1))\n",
        "        padding = torch.zeros(x.size(0), (self.win_size - 1) // 2, x.size(-1), device=x.device)\n",
        "        x_pad = torch.cat((padding, x, padding), 1).unsqueeze(1)\n",
        "\n",
        "        scores = self.attention_layer(x_pad).squeeze(1)\n",
        "        out = torch.mul(x, scores).unsqueeze(1)\n",
        "        out = self.cnn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class GlobalAttention(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, out_channels):\n",
        "        super(GlobalAttention, self).__init__()\n",
        "\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 1, kernel_size=(input_size, embed_size)),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.cnn_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(2, embed_size)),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size - 2 + 1, 1)))\n",
        "        self.cnn_2 = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(3, embed_size)),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size - 3 + 1, 1)))\n",
        "        self.cnn_3 = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(4, embed_size)),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size - 4 + 1, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        score = self.attention_layer(x)\n",
        "        out = torch.mul(x, score)\n",
        "        return self.cnn_1(out), self.cnn_2(out), self.cnn_3(out)\n",
        "\n",
        "\n",
        "class CNNDLGA(nn.Module):\n",
        "    def __init__(self, input_size, embed_size=100, win_size=5, channels_local=200, channels_global=100,\n",
        "                 hidden_size=500, output_size=50):\n",
        "        super(CNNDLGA, self).__init__()\n",
        "\n",
        "        self.localAttentionLayer_user = LocalAttention(input_size, embed_size, win_size, channels_local)\n",
        "        self.globalAttentionLayer_user = GlobalAttention(input_size, embed_size, channels_global)\n",
        "        self.localAttentionLayer_item = LocalAttention(input_size, embed_size, win_size, channels_local)\n",
        "        self.globalAttentionLayer_item = GlobalAttention(input_size, embed_size, channels_global)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fcLayer = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),  # 입력 차원을 명시하지 않음\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_user, x_item):\n",
        "        # User side\n",
        "        local_user = self.localAttentionLayer_user(x_user)\n",
        "        global1_user, global2_user, global3_user = self.globalAttentionLayer_user(x_user)\n",
        "        out_user = torch.cat((local_user, global1_user, global2_user, global3_user), 1).view(x_user.size(0), -1)\n",
        "        out_user = self.fcLayer(out_user)\n",
        "\n",
        "        # Item side\n",
        "        local_item = self.localAttentionLayer_item(x_item)\n",
        "        global1_item, global2_item, global3_item = self.globalAttentionLayer_item(x_item)\n",
        "        out_item = torch.cat((local_item, global1_item, global2_item, global3_item), 1).view(x_item.size(0), -1)\n",
        "        out_item = self.fcLayer(out_item)\n",
        "\n",
        "        # Combine user and item representations\n",
        "        out = torch.sum(torch.mul(out_user, out_item), 1)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "P9l84Y7bWnKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Result"
      ],
      "metadata": {
        "id": "GthbzHaXjqBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# ================= Hyperparameters =================\n",
        "input_size = 10000        # 리뷰 길이\n",
        "embed_size = 64           # 임베딩 차원\n",
        "num_epochs = 10           # 학습 에폭 수\n",
        "batch_size = 32           # 배치 사이즈\n",
        "learning_rate = 1e-4      # 학습률\n",
        "hidden_size = 500         # FC 레이어 히든 사이즈\n",
        "output_size = 1           # 출력 사이즈 (평점 예측)\n",
        "win_size = 5              # 윈도우 크기\n",
        "channels_local = 200      # LocalAttention 채널 수\n",
        "channels_global = 100     # GlobalAttention 채널 수\n",
        "\n",
        "# ================= Utility Functions =================\n",
        "#def evaluation(target, cf_out):\n",
        "#    \"\"\"AUC 계산\"\"\"\n",
        "#    fpr, tpr, _ = metrics.roc_curve(target, cf_out)\n",
        "#    auc = metrics.auc(fpr, tpr)\n",
        "#    return auc\n",
        "\n",
        "def to_var(x):\n",
        "    \"\"\"CUDA 변환 함수\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x)\n",
        "\n",
        "# ================= Loading Data =================\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = get_loader(train_path, emb_path, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = get_loader(valid_path, emb_path, batch_size=batch_size, shuffle=False)\n",
        "test_loader = get_loader(test_path, emb_path, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"train/valid/test: {len(train_loader)}/{len(valid_loader)}/{len(test_loader)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ================= Model Initialization =================\n",
        "model = CNNDLGA(input_size=input_size, embed_size=embed_size, win_size=win_size,\n",
        "                channels_local=channels_local, channels_global=channels_global,\n",
        "                hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "# 손실 함수 및 최적화기\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Training Start..\")\n",
        "\n",
        "# ================= Training Loop =================\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    batch_loss = 0.0\n",
        "\n",
        "    for i, (user_review, item_review, labels) in enumerate(train_loader):\n",
        "        # 데이터 CUDA 변환\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "        labels = to_var(labels)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(user_review, item_review)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and Optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], \"\n",
        "                  f\"Loss: {batch_loss / 10:.4f}\")\n",
        "            batch_loss = 0.0\n",
        "\n",
        "    # 모델 저장\n",
        "    #torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pkl\")\n",
        "    save_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/D-attn/model save/'  # Google Drive 저장 경로\n",
        "    torch.save(model.state_dict(), f\"{save_path}model_epoch_{epoch+1}.pkl\")\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Training End..\")\n",
        "\n",
        "# ================= Validation Loop =================\n",
        "\n",
        "\n",
        "\n",
        "print(\"Validation Start...\")\n",
        "model.eval()\n",
        "all_outputs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user_review, item_review, labels in valid_loader:\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "        labels = to_var(labels)\n",
        "\n",
        "        outputs = model(user_review, item_review)\n",
        "        all_outputs.extend(outputs.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# AUC 계산\n",
        "#auc = evaluation(all_labels, all_outputs)\n",
        "#print(f\"Validation AUC: {auc:.4f}\")\n",
        "#print(\"=\" * 80)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ================= Testing Loop =================\n",
        "print(\"Testing Start...\")\n",
        "all_outputs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user_review, item_review, labels in test_loader:\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "\n",
        "        outputs = model(user_review, item_review)\n",
        "        all_outputs.extend(outputs.cpu().numpy())\n",
        "\n",
        "# 결과 저장\n",
        "with open(\"/content/drive/MyDrive/IDS/amaxon reviews 2023/D-attn/result_test.pickle\", \"wb\") as f:\n",
        "    pickle.dump(all_outputs, f)\n",
        "\n",
        "print(\"Testing End. Results saved to 'result_test.pickle'.\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lgtxwznMnvmM",
        "outputId": "8c9cc688-9831-46ea-8b4b-45cc7715988a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "train/valid/test: 400/50/50\n",
            "================================================================================\n",
            "================================================================================\n",
            "Training Start..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/400], Loss: 16.9719\n",
            "Epoch [1/10], Step [20/400], Loss: 16.4707\n",
            "Epoch [1/10], Step [30/400], Loss: 15.9221\n",
            "Epoch [1/10], Step [40/400], Loss: 15.3367\n",
            "Epoch [1/10], Step [50/400], Loss: 14.5974\n",
            "Epoch [1/10], Step [60/400], Loss: 13.5155\n",
            "Epoch [1/10], Step [70/400], Loss: 10.3707\n",
            "Epoch [1/10], Step [80/400], Loss: 7.6027\n",
            "Epoch [1/10], Step [90/400], Loss: 5.1182\n",
            "Epoch [1/10], Step [100/400], Loss: 2.9699\n",
            "Epoch [1/10], Step [110/400], Loss: 2.5935\n",
            "Epoch [1/10], Step [120/400], Loss: 3.0695\n",
            "Epoch [1/10], Step [130/400], Loss: 2.5003\n",
            "Epoch [1/10], Step [140/400], Loss: 2.5309\n",
            "Epoch [1/10], Step [150/400], Loss: 2.7779\n",
            "Epoch [1/10], Step [160/400], Loss: 2.7100\n",
            "Epoch [1/10], Step [170/400], Loss: 2.5811\n",
            "Epoch [1/10], Step [180/400], Loss: 2.8049\n",
            "Epoch [1/10], Step [190/400], Loss: 2.4692\n",
            "Epoch [1/10], Step [200/400], Loss: 2.5675\n",
            "Epoch [1/10], Step [210/400], Loss: 2.3837\n",
            "Epoch [1/10], Step [220/400], Loss: 2.5475\n",
            "Epoch [1/10], Step [230/400], Loss: 2.5970\n",
            "Epoch [1/10], Step [240/400], Loss: 2.4234\n",
            "Epoch [1/10], Step [250/400], Loss: 2.6648\n",
            "Epoch [1/10], Step [260/400], Loss: 2.6300\n",
            "Epoch [1/10], Step [270/400], Loss: 2.5522\n",
            "Epoch [1/10], Step [280/400], Loss: 2.8379\n",
            "Epoch [1/10], Step [290/400], Loss: 2.6168\n",
            "Epoch [1/10], Step [300/400], Loss: 2.6691\n",
            "Epoch [1/10], Step [310/400], Loss: 2.5477\n",
            "Epoch [1/10], Step [320/400], Loss: 2.6631\n",
            "Epoch [1/10], Step [330/400], Loss: 2.4141\n",
            "Epoch [1/10], Step [340/400], Loss: 2.5204\n",
            "Epoch [1/10], Step [350/400], Loss: 2.4649\n",
            "Epoch [1/10], Step [360/400], Loss: 2.8497\n",
            "Epoch [1/10], Step [370/400], Loss: 2.3884\n",
            "Epoch [1/10], Step [380/400], Loss: 2.4223\n",
            "Epoch [1/10], Step [390/400], Loss: 2.6027\n",
            "Epoch [1/10], Step [400/400], Loss: 2.6468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Step [10/400], Loss: 2.5004\n",
            "Epoch [2/10], Step [20/400], Loss: 2.6830\n",
            "Epoch [2/10], Step [30/400], Loss: 2.5208\n",
            "Epoch [2/10], Step [40/400], Loss: 2.5213\n",
            "Epoch [2/10], Step [50/400], Loss: 2.6493\n",
            "Epoch [2/10], Step [60/400], Loss: 2.2098\n",
            "Epoch [2/10], Step [70/400], Loss: 2.4943\n",
            "Epoch [2/10], Step [80/400], Loss: 2.5051\n",
            "Epoch [2/10], Step [90/400], Loss: 2.7184\n",
            "Epoch [2/10], Step [100/400], Loss: 2.4395\n",
            "Epoch [2/10], Step [110/400], Loss: 2.7811\n",
            "Epoch [2/10], Step [120/400], Loss: 2.5636\n",
            "Epoch [2/10], Step [130/400], Loss: 2.6296\n",
            "Epoch [2/10], Step [140/400], Loss: 2.6774\n",
            "Epoch [2/10], Step [150/400], Loss: 2.5141\n",
            "Epoch [2/10], Step [160/400], Loss: 2.6182\n",
            "Epoch [2/10], Step [170/400], Loss: 2.6590\n",
            "Epoch [2/10], Step [180/400], Loss: 2.6332\n",
            "Epoch [2/10], Step [190/400], Loss: 2.6039\n",
            "Epoch [2/10], Step [200/400], Loss: 2.6499\n",
            "Epoch [2/10], Step [210/400], Loss: 2.5344\n",
            "Epoch [2/10], Step [220/400], Loss: 2.7686\n",
            "Epoch [2/10], Step [230/400], Loss: 2.4972\n",
            "Epoch [2/10], Step [240/400], Loss: 2.4428\n",
            "Epoch [2/10], Step [250/400], Loss: 2.3032\n",
            "Epoch [2/10], Step [260/400], Loss: 2.5383\n",
            "Epoch [2/10], Step [270/400], Loss: 2.6528\n",
            "Epoch [2/10], Step [280/400], Loss: 2.5745\n",
            "Epoch [2/10], Step [290/400], Loss: 2.4251\n",
            "Epoch [2/10], Step [300/400], Loss: 2.4596\n",
            "Epoch [2/10], Step [310/400], Loss: 2.7240\n",
            "Epoch [2/10], Step [320/400], Loss: 2.7338\n",
            "Epoch [2/10], Step [330/400], Loss: 2.4829\n",
            "Epoch [2/10], Step [340/400], Loss: 2.5409\n",
            "Epoch [2/10], Step [350/400], Loss: 2.4787\n",
            "Epoch [2/10], Step [360/400], Loss: 2.6847\n",
            "Epoch [2/10], Step [370/400], Loss: 2.5858\n",
            "Epoch [2/10], Step [380/400], Loss: 2.4860\n",
            "Epoch [2/10], Step [390/400], Loss: 2.3996\n",
            "Epoch [2/10], Step [400/400], Loss: 2.7411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Step [10/400], Loss: 2.4954\n",
            "Epoch [3/10], Step [20/400], Loss: 2.5110\n",
            "Epoch [3/10], Step [30/400], Loss: 2.3518\n",
            "Epoch [3/10], Step [40/400], Loss: 2.6719\n",
            "Epoch [3/10], Step [50/400], Loss: 2.4759\n",
            "Epoch [3/10], Step [60/400], Loss: 2.5939\n",
            "Epoch [3/10], Step [70/400], Loss: 2.4971\n",
            "Epoch [3/10], Step [80/400], Loss: 2.3794\n",
            "Epoch [3/10], Step [90/400], Loss: 2.5468\n",
            "Epoch [3/10], Step [100/400], Loss: 2.4520\n",
            "Epoch [3/10], Step [110/400], Loss: 2.7136\n",
            "Epoch [3/10], Step [120/400], Loss: 2.7511\n",
            "Epoch [3/10], Step [130/400], Loss: 2.6022\n",
            "Epoch [3/10], Step [140/400], Loss: 2.6916\n",
            "Epoch [3/10], Step [150/400], Loss: 2.5992\n",
            "Epoch [3/10], Step [160/400], Loss: 2.5545\n",
            "Epoch [3/10], Step [170/400], Loss: 2.5099\n",
            "Epoch [3/10], Step [180/400], Loss: 2.5854\n",
            "Epoch [3/10], Step [190/400], Loss: 2.7476\n",
            "Epoch [3/10], Step [200/400], Loss: 2.6408\n",
            "Epoch [3/10], Step [210/400], Loss: 2.5570\n",
            "Epoch [3/10], Step [220/400], Loss: 2.5072\n",
            "Epoch [3/10], Step [230/400], Loss: 2.7418\n",
            "Epoch [3/10], Step [240/400], Loss: 2.8254\n",
            "Epoch [3/10], Step [250/400], Loss: 2.7946\n",
            "Epoch [3/10], Step [260/400], Loss: 2.2402\n",
            "Epoch [3/10], Step [270/400], Loss: 2.4286\n",
            "Epoch [3/10], Step [280/400], Loss: 2.3442\n",
            "Epoch [3/10], Step [290/400], Loss: 2.6249\n",
            "Epoch [3/10], Step [300/400], Loss: 2.3391\n",
            "Epoch [3/10], Step [310/400], Loss: 2.2725\n",
            "Epoch [3/10], Step [320/400], Loss: 2.7755\n",
            "Epoch [3/10], Step [330/400], Loss: 2.7841\n",
            "Epoch [3/10], Step [340/400], Loss: 2.7374\n",
            "Epoch [3/10], Step [350/400], Loss: 2.5063\n",
            "Epoch [3/10], Step [360/400], Loss: 2.5440\n",
            "Epoch [3/10], Step [370/400], Loss: 2.3798\n",
            "Epoch [3/10], Step [380/400], Loss: 2.6238\n",
            "Epoch [3/10], Step [390/400], Loss: 2.5136\n",
            "Epoch [3/10], Step [400/400], Loss: 2.4362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Step [10/400], Loss: 2.8671\n",
            "Epoch [4/10], Step [20/400], Loss: 2.3851\n",
            "Epoch [4/10], Step [30/400], Loss: 2.2614\n",
            "Epoch [4/10], Step [40/400], Loss: 2.3677\n",
            "Epoch [4/10], Step [50/400], Loss: 2.5357\n",
            "Epoch [4/10], Step [60/400], Loss: 2.6519\n",
            "Epoch [4/10], Step [70/400], Loss: 2.5575\n",
            "Epoch [4/10], Step [80/400], Loss: 2.6180\n",
            "Epoch [4/10], Step [90/400], Loss: 2.7396\n",
            "Epoch [4/10], Step [100/400], Loss: 2.5483\n",
            "Epoch [4/10], Step [110/400], Loss: 2.3803\n",
            "Epoch [4/10], Step [120/400], Loss: 2.6225\n",
            "Epoch [4/10], Step [130/400], Loss: 2.3887\n",
            "Epoch [4/10], Step [140/400], Loss: 2.3568\n",
            "Epoch [4/10], Step [150/400], Loss: 2.4316\n",
            "Epoch [4/10], Step [160/400], Loss: 2.7848\n",
            "Epoch [4/10], Step [170/400], Loss: 2.6482\n",
            "Epoch [4/10], Step [180/400], Loss: 2.7590\n",
            "Epoch [4/10], Step [190/400], Loss: 2.4920\n",
            "Epoch [4/10], Step [200/400], Loss: 2.6109\n",
            "Epoch [4/10], Step [210/400], Loss: 2.6399\n",
            "Epoch [4/10], Step [220/400], Loss: 2.5439\n",
            "Epoch [4/10], Step [230/400], Loss: 2.4499\n",
            "Epoch [4/10], Step [240/400], Loss: 2.4869\n",
            "Epoch [4/10], Step [250/400], Loss: 2.4704\n",
            "Epoch [4/10], Step [260/400], Loss: 2.5477\n",
            "Epoch [4/10], Step [270/400], Loss: 2.3452\n",
            "Epoch [4/10], Step [280/400], Loss: 2.5505\n",
            "Epoch [4/10], Step [290/400], Loss: 2.7831\n",
            "Epoch [4/10], Step [300/400], Loss: 2.8087\n",
            "Epoch [4/10], Step [310/400], Loss: 2.4994\n",
            "Epoch [4/10], Step [320/400], Loss: 2.5782\n",
            "Epoch [4/10], Step [330/400], Loss: 2.3042\n",
            "Epoch [4/10], Step [340/400], Loss: 2.7363\n",
            "Epoch [4/10], Step [350/400], Loss: 2.6453\n",
            "Epoch [4/10], Step [360/400], Loss: 2.3640\n",
            "Epoch [4/10], Step [370/400], Loss: 2.4733\n",
            "Epoch [4/10], Step [380/400], Loss: 2.6887\n",
            "Epoch [4/10], Step [390/400], Loss: 2.4982\n",
            "Epoch [4/10], Step [400/400], Loss: 2.2948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Step [10/400], Loss: 2.7920\n",
            "Epoch [5/10], Step [20/400], Loss: 2.5345\n",
            "Epoch [5/10], Step [30/400], Loss: 2.5291\n",
            "Epoch [5/10], Step [40/400], Loss: 2.5610\n",
            "Epoch [5/10], Step [50/400], Loss: 2.3967\n",
            "Epoch [5/10], Step [60/400], Loss: 2.6377\n",
            "Epoch [5/10], Step [70/400], Loss: 2.6743\n",
            "Epoch [5/10], Step [80/400], Loss: 2.6320\n",
            "Epoch [5/10], Step [90/400], Loss: 2.5558\n",
            "Epoch [5/10], Step [100/400], Loss: 2.4557\n",
            "Epoch [5/10], Step [110/400], Loss: 2.5996\n",
            "Epoch [5/10], Step [120/400], Loss: 2.4851\n",
            "Epoch [5/10], Step [130/400], Loss: 2.8017\n",
            "Epoch [5/10], Step [140/400], Loss: 2.3647\n",
            "Epoch [5/10], Step [150/400], Loss: 2.5515\n",
            "Epoch [5/10], Step [160/400], Loss: 2.5877\n",
            "Epoch [5/10], Step [170/400], Loss: 2.5121\n",
            "Epoch [5/10], Step [180/400], Loss: 2.4652\n",
            "Epoch [5/10], Step [190/400], Loss: 2.3947\n",
            "Epoch [5/10], Step [200/400], Loss: 2.3650\n",
            "Epoch [5/10], Step [210/400], Loss: 2.3736\n",
            "Epoch [5/10], Step [220/400], Loss: 2.5626\n",
            "Epoch [5/10], Step [230/400], Loss: 2.7639\n",
            "Epoch [5/10], Step [240/400], Loss: 2.5982\n",
            "Epoch [5/10], Step [250/400], Loss: 2.4402\n",
            "Epoch [5/10], Step [260/400], Loss: 2.6700\n",
            "Epoch [5/10], Step [270/400], Loss: 2.7773\n",
            "Epoch [5/10], Step [280/400], Loss: 2.7645\n",
            "Epoch [5/10], Step [290/400], Loss: 2.3619\n",
            "Epoch [5/10], Step [300/400], Loss: 2.6709\n",
            "Epoch [5/10], Step [310/400], Loss: 2.6905\n",
            "Epoch [5/10], Step [320/400], Loss: 2.7049\n",
            "Epoch [5/10], Step [330/400], Loss: 2.3468\n",
            "Epoch [5/10], Step [340/400], Loss: 2.6227\n",
            "Epoch [5/10], Step [350/400], Loss: 2.7858\n",
            "Epoch [5/10], Step [360/400], Loss: 2.7128\n",
            "Epoch [5/10], Step [370/400], Loss: 2.4712\n",
            "Epoch [5/10], Step [380/400], Loss: 2.4852\n",
            "Epoch [5/10], Step [390/400], Loss: 2.3312\n",
            "Epoch [5/10], Step [400/400], Loss: 2.5342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Step [10/400], Loss: 2.7887\n",
            "Epoch [6/10], Step [20/400], Loss: 2.7064\n",
            "Epoch [6/10], Step [30/400], Loss: 2.3826\n",
            "Epoch [6/10], Step [40/400], Loss: 2.7219\n",
            "Epoch [6/10], Step [50/400], Loss: 2.6864\n",
            "Epoch [6/10], Step [60/400], Loss: 2.1604\n",
            "Epoch [6/10], Step [70/400], Loss: 2.6555\n",
            "Epoch [6/10], Step [80/400], Loss: 2.6410\n",
            "Epoch [6/10], Step [90/400], Loss: 2.8095\n",
            "Epoch [6/10], Step [100/400], Loss: 2.5176\n",
            "Epoch [6/10], Step [110/400], Loss: 2.5242\n",
            "Epoch [6/10], Step [120/400], Loss: 2.4716\n",
            "Epoch [6/10], Step [130/400], Loss: 2.3612\n",
            "Epoch [6/10], Step [140/400], Loss: 2.5074\n",
            "Epoch [6/10], Step [150/400], Loss: 2.6473\n",
            "Epoch [6/10], Step [160/400], Loss: 2.7594\n",
            "Epoch [6/10], Step [170/400], Loss: 2.6225\n",
            "Epoch [6/10], Step [180/400], Loss: 2.4537\n",
            "Epoch [6/10], Step [190/400], Loss: 2.5978\n",
            "Epoch [6/10], Step [200/400], Loss: 2.1164\n",
            "Epoch [6/10], Step [210/400], Loss: 2.5291\n",
            "Epoch [6/10], Step [220/400], Loss: 2.7025\n",
            "Epoch [6/10], Step [230/400], Loss: 2.5307\n",
            "Epoch [6/10], Step [240/400], Loss: 2.3100\n",
            "Epoch [6/10], Step [250/400], Loss: 2.5891\n",
            "Epoch [6/10], Step [260/400], Loss: 2.6141\n",
            "Epoch [6/10], Step [270/400], Loss: 2.5632\n",
            "Epoch [6/10], Step [280/400], Loss: 2.6683\n",
            "Epoch [6/10], Step [290/400], Loss: 2.4202\n",
            "Epoch [6/10], Step [300/400], Loss: 2.5854\n",
            "Epoch [6/10], Step [310/400], Loss: 2.4727\n",
            "Epoch [6/10], Step [320/400], Loss: 2.8134\n",
            "Epoch [6/10], Step [330/400], Loss: 2.5916\n",
            "Epoch [6/10], Step [340/400], Loss: 2.5276\n",
            "Epoch [6/10], Step [350/400], Loss: 2.2249\n",
            "Epoch [6/10], Step [360/400], Loss: 2.5581\n",
            "Epoch [6/10], Step [370/400], Loss: 2.5187\n",
            "Epoch [6/10], Step [380/400], Loss: 2.6555\n",
            "Epoch [6/10], Step [390/400], Loss: 2.6945\n",
            "Epoch [6/10], Step [400/400], Loss: 2.5438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Step [10/400], Loss: 2.4855\n",
            "Epoch [7/10], Step [20/400], Loss: 2.5413\n",
            "Epoch [7/10], Step [30/400], Loss: 2.4174\n",
            "Epoch [7/10], Step [40/400], Loss: 2.2779\n",
            "Epoch [7/10], Step [50/400], Loss: 2.5488\n",
            "Epoch [7/10], Step [60/400], Loss: 2.6491\n",
            "Epoch [7/10], Step [70/400], Loss: 2.5118\n",
            "Epoch [7/10], Step [80/400], Loss: 2.4463\n",
            "Epoch [7/10], Step [90/400], Loss: 2.5541\n",
            "Epoch [7/10], Step [100/400], Loss: 2.5161\n",
            "Epoch [7/10], Step [110/400], Loss: 2.3879\n",
            "Epoch [7/10], Step [120/400], Loss: 2.5269\n",
            "Epoch [7/10], Step [130/400], Loss: 2.5490\n",
            "Epoch [7/10], Step [140/400], Loss: 2.6187\n",
            "Epoch [7/10], Step [150/400], Loss: 2.8513\n",
            "Epoch [7/10], Step [160/400], Loss: 2.5192\n",
            "Epoch [7/10], Step [170/400], Loss: 2.5944\n",
            "Epoch [7/10], Step [180/400], Loss: 2.3400\n",
            "Epoch [7/10], Step [190/400], Loss: 2.3331\n",
            "Epoch [7/10], Step [200/400], Loss: 2.7144\n",
            "Epoch [7/10], Step [210/400], Loss: 2.9153\n",
            "Epoch [7/10], Step [220/400], Loss: 2.5199\n",
            "Epoch [7/10], Step [230/400], Loss: 2.5488\n",
            "Epoch [7/10], Step [240/400], Loss: 2.4802\n",
            "Epoch [7/10], Step [250/400], Loss: 2.5264\n",
            "Epoch [7/10], Step [260/400], Loss: 2.5354\n",
            "Epoch [7/10], Step [270/400], Loss: 2.6547\n",
            "Epoch [7/10], Step [280/400], Loss: 2.5495\n",
            "Epoch [7/10], Step [290/400], Loss: 2.6395\n",
            "Epoch [7/10], Step [300/400], Loss: 2.6823\n",
            "Epoch [7/10], Step [310/400], Loss: 2.4576\n",
            "Epoch [7/10], Step [320/400], Loss: 2.4249\n",
            "Epoch [7/10], Step [330/400], Loss: 2.9039\n",
            "Epoch [7/10], Step [340/400], Loss: 2.4376\n",
            "Epoch [7/10], Step [350/400], Loss: 2.2825\n",
            "Epoch [7/10], Step [360/400], Loss: 2.5805\n",
            "Epoch [7/10], Step [370/400], Loss: 2.5065\n",
            "Epoch [7/10], Step [380/400], Loss: 2.4095\n",
            "Epoch [7/10], Step [390/400], Loss: 2.4452\n",
            "Epoch [7/10], Step [400/400], Loss: 2.3069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Step [10/400], Loss: 2.5210\n",
            "Epoch [8/10], Step [20/400], Loss: 2.4522\n",
            "Epoch [8/10], Step [30/400], Loss: 2.6134\n",
            "Epoch [8/10], Step [40/400], Loss: 2.4135\n",
            "Epoch [8/10], Step [50/400], Loss: 2.3684\n",
            "Epoch [8/10], Step [60/400], Loss: 2.6005\n",
            "Epoch [8/10], Step [70/400], Loss: 2.2644\n",
            "Epoch [8/10], Step [80/400], Loss: 2.5478\n",
            "Epoch [8/10], Step [90/400], Loss: 2.5772\n",
            "Epoch [8/10], Step [100/400], Loss: 2.5443\n",
            "Epoch [8/10], Step [110/400], Loss: 2.4912\n",
            "Epoch [8/10], Step [120/400], Loss: 2.7153\n",
            "Epoch [8/10], Step [130/400], Loss: 2.5272\n",
            "Epoch [8/10], Step [140/400], Loss: 2.8694\n",
            "Epoch [8/10], Step [150/400], Loss: 2.7870\n",
            "Epoch [8/10], Step [160/400], Loss: 2.4028\n",
            "Epoch [8/10], Step [170/400], Loss: 2.6353\n",
            "Epoch [8/10], Step [180/400], Loss: 2.6029\n",
            "Epoch [8/10], Step [190/400], Loss: 2.6796\n",
            "Epoch [8/10], Step [200/400], Loss: 2.3671\n",
            "Epoch [8/10], Step [210/400], Loss: 2.6655\n",
            "Epoch [8/10], Step [220/400], Loss: 2.5226\n",
            "Epoch [8/10], Step [230/400], Loss: 2.2463\n",
            "Epoch [8/10], Step [240/400], Loss: 2.4701\n",
            "Epoch [8/10], Step [250/400], Loss: 2.3915\n",
            "Epoch [8/10], Step [260/400], Loss: 2.6928\n",
            "Epoch [8/10], Step [270/400], Loss: 2.6812\n",
            "Epoch [8/10], Step [280/400], Loss: 2.7451\n",
            "Epoch [8/10], Step [290/400], Loss: 2.3679\n",
            "Epoch [8/10], Step [300/400], Loss: 2.5580\n",
            "Epoch [8/10], Step [310/400], Loss: 2.4712\n",
            "Epoch [8/10], Step [320/400], Loss: 2.6398\n",
            "Epoch [8/10], Step [330/400], Loss: 2.4332\n",
            "Epoch [8/10], Step [340/400], Loss: 2.6854\n",
            "Epoch [8/10], Step [350/400], Loss: 2.5727\n",
            "Epoch [8/10], Step [360/400], Loss: 2.5471\n",
            "Epoch [8/10], Step [370/400], Loss: 2.7138\n",
            "Epoch [8/10], Step [380/400], Loss: 2.5803\n",
            "Epoch [8/10], Step [390/400], Loss: 2.5929\n",
            "Epoch [8/10], Step [400/400], Loss: 2.4807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Step [10/400], Loss: 2.4467\n",
            "Epoch [9/10], Step [20/400], Loss: 2.6405\n",
            "Epoch [9/10], Step [30/400], Loss: 2.5250\n",
            "Epoch [9/10], Step [40/400], Loss: 2.5490\n",
            "Epoch [9/10], Step [50/400], Loss: 2.4500\n",
            "Epoch [9/10], Step [60/400], Loss: 2.5031\n",
            "Epoch [9/10], Step [70/400], Loss: 2.5616\n",
            "Epoch [9/10], Step [80/400], Loss: 2.4595\n",
            "Epoch [9/10], Step [90/400], Loss: 2.3687\n",
            "Epoch [9/10], Step [100/400], Loss: 2.7539\n",
            "Epoch [9/10], Step [110/400], Loss: 2.6041\n",
            "Epoch [9/10], Step [120/400], Loss: 2.3762\n",
            "Epoch [9/10], Step [130/400], Loss: 2.1874\n",
            "Epoch [9/10], Step [140/400], Loss: 2.6136\n",
            "Epoch [9/10], Step [150/400], Loss: 2.5484\n",
            "Epoch [9/10], Step [160/400], Loss: 2.4921\n",
            "Epoch [9/10], Step [170/400], Loss: 2.7805\n",
            "Epoch [9/10], Step [180/400], Loss: 2.6155\n",
            "Epoch [9/10], Step [190/400], Loss: 2.3444\n",
            "Epoch [9/10], Step [200/400], Loss: 2.6070\n",
            "Epoch [9/10], Step [210/400], Loss: 2.4321\n",
            "Epoch [9/10], Step [220/400], Loss: 2.4007\n",
            "Epoch [9/10], Step [230/400], Loss: 2.5884\n",
            "Epoch [9/10], Step [240/400], Loss: 2.5220\n",
            "Epoch [9/10], Step [250/400], Loss: 2.6370\n",
            "Epoch [9/10], Step [260/400], Loss: 2.8622\n",
            "Epoch [9/10], Step [270/400], Loss: 2.5330\n",
            "Epoch [9/10], Step [280/400], Loss: 2.3426\n",
            "Epoch [9/10], Step [290/400], Loss: 2.2868\n",
            "Epoch [9/10], Step [300/400], Loss: 2.6411\n",
            "Epoch [9/10], Step [310/400], Loss: 2.5563\n",
            "Epoch [9/10], Step [320/400], Loss: 2.7968\n",
            "Epoch [9/10], Step [330/400], Loss: 2.4651\n",
            "Epoch [9/10], Step [340/400], Loss: 2.6366\n",
            "Epoch [9/10], Step [350/400], Loss: 2.4648\n",
            "Epoch [9/10], Step [360/400], Loss: 2.5195\n",
            "Epoch [9/10], Step [370/400], Loss: 2.6397\n",
            "Epoch [9/10], Step [380/400], Loss: 2.4339\n",
            "Epoch [9/10], Step [390/400], Loss: 2.6890\n",
            "Epoch [9/10], Step [400/400], Loss: 2.7851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Step [10/400], Loss: 2.5129\n",
            "Epoch [10/10], Step [20/400], Loss: 2.4462\n",
            "Epoch [10/10], Step [30/400], Loss: 2.5412\n",
            "Epoch [10/10], Step [40/400], Loss: 2.3659\n",
            "Epoch [10/10], Step [50/400], Loss: 2.8020\n",
            "Epoch [10/10], Step [60/400], Loss: 2.6062\n",
            "Epoch [10/10], Step [70/400], Loss: 2.6103\n",
            "Epoch [10/10], Step [80/400], Loss: 2.7646\n",
            "Epoch [10/10], Step [90/400], Loss: 2.4894\n",
            "Epoch [10/10], Step [100/400], Loss: 2.4823\n",
            "Epoch [10/10], Step [110/400], Loss: 2.6961\n",
            "Epoch [10/10], Step [120/400], Loss: 2.4946\n",
            "Epoch [10/10], Step [130/400], Loss: 2.4003\n",
            "Epoch [10/10], Step [140/400], Loss: 2.5464\n",
            "Epoch [10/10], Step [150/400], Loss: 2.5338\n",
            "Epoch [10/10], Step [160/400], Loss: 2.4320\n",
            "Epoch [10/10], Step [170/400], Loss: 2.7740\n",
            "Epoch [10/10], Step [180/400], Loss: 2.5422\n",
            "Epoch [10/10], Step [190/400], Loss: 2.4587\n",
            "Epoch [10/10], Step [200/400], Loss: 2.4140\n",
            "Epoch [10/10], Step [210/400], Loss: 2.3915\n",
            "Epoch [10/10], Step [220/400], Loss: 2.6157\n",
            "Epoch [10/10], Step [230/400], Loss: 2.4777\n",
            "Epoch [10/10], Step [240/400], Loss: 2.5658\n",
            "Epoch [10/10], Step [250/400], Loss: 2.6840\n",
            "Epoch [10/10], Step [260/400], Loss: 2.7573\n",
            "Epoch [10/10], Step [270/400], Loss: 2.4512\n",
            "Epoch [10/10], Step [280/400], Loss: 2.3644\n",
            "Epoch [10/10], Step [290/400], Loss: 2.3906\n",
            "Epoch [10/10], Step [300/400], Loss: 2.7255\n",
            "Epoch [10/10], Step [310/400], Loss: 2.3309\n",
            "Epoch [10/10], Step [320/400], Loss: 2.5147\n",
            "Epoch [10/10], Step [330/400], Loss: 2.3455\n",
            "Epoch [10/10], Step [340/400], Loss: 2.4781\n",
            "Epoch [10/10], Step [350/400], Loss: 2.4302\n",
            "Epoch [10/10], Step [360/400], Loss: 2.5533\n",
            "Epoch [10/10], Step [370/400], Loss: 2.4049\n",
            "Epoch [10/10], Step [380/400], Loss: 2.9526\n",
            "Epoch [10/10], Step [390/400], Loss: 2.6900\n",
            "Epoch [10/10], Step [400/400], Loss: 2.6261\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "multiclass format is not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2d4bdcdc0c61>\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# AUC 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation AUC: {auc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-2d4bdcdc0c61>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(target, cf_out)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcf_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m\"\"\"AUC 계산\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcf_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \"\"\"\n\u001b[0;32m-> 1145\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 경로\n",
        "saved_model_path = \"/content/drive/MyDrive/IDS/amaxon reviews 2023/D-attn/model save/model_epoch_10.pkl\"\n",
        "\n",
        "# 모델 인스턴스 초기화\n",
        "model = CNNDLGA(input_size=input_size, embed_size=embed_size, win_size=win_size,\n",
        "                channels_local=channels_local, channels_global=channels_global,\n",
        "                hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "# 모델 가중치 로드\n",
        "model.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "model.eval()  # 모델을 평가 모드로 전환\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVf1OqymlOxC",
        "outputId": "e8d872d7-fab0-41e6-e1cd-20763b7e8c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-825daf8748de>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(saved_model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNDLGA(\n",
              "  (localAttentionLayer_user): LocalAttention(\n",
              "    (attention_layer): Sequential(\n",
              "      (0): Conv2d(1, 1, kernel_size=(5, 64), stride=(1, 1))\n",
              "      (1): Sigmoid()\n",
              "    )\n",
              "    (cnn): Sequential(\n",
              "      (0): Conv2d(1, 200, kernel_size=(1, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(10000, 1), stride=(10000, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (globalAttentionLayer_user): GlobalAttention(\n",
              "    (attention_layer): Sequential(\n",
              "      (0): Conv2d(1, 1, kernel_size=(10000, 64), stride=(1, 1))\n",
              "      (1): Sigmoid()\n",
              "    )\n",
              "    (cnn_1): Sequential(\n",
              "      (0): Conv2d(1, 100, kernel_size=(2, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(9999, 1), stride=(9999, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (cnn_2): Sequential(\n",
              "      (0): Conv2d(1, 100, kernel_size=(3, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(9998, 1), stride=(9998, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (cnn_3): Sequential(\n",
              "      (0): Conv2d(1, 100, kernel_size=(4, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(9997, 1), stride=(9997, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (localAttentionLayer_item): LocalAttention(\n",
              "    (attention_layer): Sequential(\n",
              "      (0): Conv2d(1, 1, kernel_size=(5, 64), stride=(1, 1))\n",
              "      (1): Sigmoid()\n",
              "    )\n",
              "    (cnn): Sequential(\n",
              "      (0): Conv2d(1, 200, kernel_size=(1, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(10000, 1), stride=(10000, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (globalAttentionLayer_item): GlobalAttention(\n",
              "    (attention_layer): Sequential(\n",
              "      (0): Conv2d(1, 1, kernel_size=(10000, 64), stride=(1, 1))\n",
              "      (1): Sigmoid()\n",
              "    )\n",
              "    (cnn_1): Sequential(\n",
              "      (0): Conv2d(1, 100, kernel_size=(2, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(9999, 1), stride=(9999, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (cnn_2): Sequential(\n",
              "      (0): Conv2d(1, 100, kernel_size=(3, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(9998, 1), stride=(9998, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (cnn_3): Sequential(\n",
              "      (0): Conv2d(1, 100, kernel_size=(4, 64), stride=(1, 1))\n",
              "      (1): Tanh()\n",
              "      (2): MaxPool2d(kernel_size=(9997, 1), stride=(9997, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (fcLayer): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=500, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Validation Loop =================\n",
        "print(\"Validation Start...\")\n",
        "model.eval()\n",
        "batch_loss = 0.0  # Validation Loss 누적 변수\n",
        "total_step = len(valid_loader)  # Validation 배치 개수\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (user_review, item_review, labels) in enumerate(valid_loader):\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "        labels = to_var(labels)\n",
        "\n",
        "        outputs = model(user_review, item_review)\n",
        "        loss = criterion(outputs, labels)  # Validation 손실 계산\n",
        "        batch_loss += loss.item()\n",
        "\n",
        "        # Validation Step별 손실 출력 (선택 사항)\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Validation Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 전체 Validation 손실 평균 출력\n",
        "avg_loss = batch_loss / total_step\n",
        "print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYk2T2ATmX_s",
        "outputId": "dc3821dd-f466-4a33-bee7-b0be6ae0029d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Step [10/50], Loss: 2.5804\n",
            "Validation Step [20/50], Loss: 3.1635\n",
            "Validation Step [30/50], Loss: 2.6094\n",
            "Validation Step [40/50], Loss: 2.1512\n",
            "Validation Step [50/50], Loss: 2.7194\n",
            "Validation Loss: 2.4715\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Testing Loop =================\n",
        "print(\"Testing Start...\")\n",
        "test_outputs = []\n",
        "test_labels = []\n",
        "test_loss = 0.0\n",
        "\n",
        "model.eval()  # Evaluation 모드로 설정\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (user_review, item_review, labels) in enumerate(test_loader):\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "        labels = to_var(labels)\n",
        "\n",
        "        # Forward 예측\n",
        "        outputs = model(user_review, item_review)\n",
        "\n",
        "        # MSE 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # 배치별 MSE 출력\n",
        "        print(f\"Test Step [{batch_idx+1}/{len(test_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # 예측값과 실제값 저장\n",
        "        test_outputs.extend(outputs.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 평균 MSE 계산\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# 결과 저장\n",
        "with open(\"/content/drive/MyDrive/IDS/amaxon reviews 2023/D-attn/re-eval_result_test.pickle\", \"wb\") as f:\n",
        "    pickle.dump(test_outputs, f)\n",
        "\n",
        "print(f\"Testing End. Results saved to 're-eval_result_test.pickle'.\")\n",
        "print(f\"Test Average MSE: {test_loss:.4f}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2AxKRXDqI0k",
        "outputId": "eda0050e-e889-4d8b-e811-edffbc2ba767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Step [1/50], Loss: 2.2512\n",
            "Test Step [2/50], Loss: 2.5386\n",
            "Test Step [3/50], Loss: 1.8221\n",
            "Test Step [4/50], Loss: 2.2178\n",
            "Test Step [5/50], Loss: 2.5429\n",
            "Test Step [6/50], Loss: 2.3596\n",
            "Test Step [7/50], Loss: 2.6969\n",
            "Test Step [8/50], Loss: 3.0510\n",
            "Test Step [9/50], Loss: 3.3425\n",
            "Test Step [10/50], Loss: 2.4804\n",
            "Test Step [11/50], Loss: 2.7344\n",
            "Test Step [12/50], Loss: 2.0262\n",
            "Test Step [13/50], Loss: 3.2093\n",
            "Test Step [14/50], Loss: 2.5304\n",
            "Test Step [15/50], Loss: 2.2346\n",
            "Test Step [16/50], Loss: 2.0761\n",
            "Test Step [17/50], Loss: 1.7263\n",
            "Test Step [18/50], Loss: 2.5386\n",
            "Test Step [19/50], Loss: 2.4761\n",
            "Test Step [20/50], Loss: 2.2054\n",
            "Test Step [21/50], Loss: 1.7512\n",
            "Test Step [22/50], Loss: 2.1179\n",
            "Test Step [23/50], Loss: 2.9677\n",
            "Test Step [24/50], Loss: 2.5053\n",
            "Test Step [25/50], Loss: 2.6261\n",
            "Test Step [26/50], Loss: 1.9471\n",
            "Test Step [27/50], Loss: 2.6760\n",
            "Test Step [28/50], Loss: 2.0595\n",
            "Test Step [29/50], Loss: 2.2804\n",
            "Test Step [30/50], Loss: 2.3554\n",
            "Test Step [31/50], Loss: 2.3720\n",
            "Test Step [32/50], Loss: 2.4053\n",
            "Test Step [33/50], Loss: 2.3345\n",
            "Test Step [34/50], Loss: 2.3886\n",
            "Test Step [35/50], Loss: 2.4095\n",
            "Test Step [36/50], Loss: 3.1176\n",
            "Test Step [37/50], Loss: 2.2596\n",
            "Test Step [38/50], Loss: 1.6805\n",
            "Test Step [39/50], Loss: 2.1679\n",
            "Test Step [40/50], Loss: 2.9969\n",
            "Test Step [41/50], Loss: 2.1638\n",
            "Test Step [42/50], Loss: 2.1679\n",
            "Test Step [43/50], Loss: 2.2470\n",
            "Test Step [44/50], Loss: 2.6220\n",
            "Test Step [45/50], Loss: 2.6762\n",
            "Test Step [46/50], Loss: 2.7509\n",
            "Test Step [47/50], Loss: 2.3054\n",
            "Test Step [48/50], Loss: 2.2969\n",
            "Test Step [49/50], Loss: 3.3260\n",
            "Test Step [50/50], Loss: 2.5177\n",
            "Testing End. Results saved to 're-eval_result_test.pickle'.\n",
            "Test Average MSE: 2.4311\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추가 분석 (RMSE와 MAE)"
      ],
      "metadata": {
        "id": "CWUNZ3nTbyek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "print(\"Validation Start...\")\n",
        "all_outputs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user_review, item_review, labels in valid_loader:\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "        labels = to_var(labels)\n",
        "\n",
        "        outputs = model(user_review, item_review)\n",
        "        all_outputs.extend(outputs.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# RMSE 및 MAE 계산\n",
        "mse = mean_squared_error(all_labels, all_outputs)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(all_labels, all_outputs)\n",
        "\n",
        "print(f\"Validation MSE: {mse:.4f}\")\n",
        "print(f\"Validation RMSE: {rmse:.4f}\")\n",
        "print(f\"Validation MAE: {mae:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ls891OrlnGT",
        "outputId": "2ab9e9c5-6928-4052-e655-f3e757ddbdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-9-9fdddab35b3b>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4714\n",
            "Validation RMSE: 1.5721\n",
            "Validation MAE: 1.4311\n"
          ]
        }
      ]
    }
  ]
}
