{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI3CNuXPIYwyzUjut6sTbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaejunchoe/HAIDS-Lab/blob/main/Upload_ver03_DeepCoNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pETm1ah3pJ_G",
        "outputId": "3c961b29-c50b-4c58-cf6b-447a5fa6c15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import inspect\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ================= Config =================\n",
        "class Config:\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_epochs = 20               # 학습 반복 수\n",
        "    batch_size = 128                # 배치 사이즈\n",
        "    learning_rate = 0.002           # 학습률 -> 가중치 업데이트 속도\n",
        "    l2_regularization = 1e-6        # L2 정규화 -> 과적합 방지 용도 / 과적합이 심하면 값을 늘리는 방식으로 수정하길 권함\n",
        "    learning_rate_decay = 0.99      # 학습률 감소 비율 -> 점진적으로 줄이면서 안정적인 학습으로 유도하기 위함\n",
        "\n",
        "    word2vec_file = '/content/drive/MyDrive/IDS/amaxon reviews 2023/glove.6B.50d.txt'\n",
        "\n",
        "    model_file = '/content/drive/MyDrive/IDS/amaxon reviews 2023/DeepCONN/model_ver02/best_model.pt'\n",
        "    train_file = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2_train.csv'\n",
        "    valid_file = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2__valid.csv'\n",
        "    test_file = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2_test.csv'\n",
        "    review_count = 10               # 최대 리뷰 수 -> 아이템 당 최대 10개의 리뷰 데이터 활용하겠다\n",
        "    review_length = 40              # 최대 40단어로 제한하겠다\n",
        "    lowest_review_count = 0\n",
        "    PAD_WORD = '<UNK>'              # 패딩 단어\n",
        "\n",
        "    kernel_count = 100              # CNN에서 필터 개수\n",
        "    kernel_size = 3                 # 필터 크기\n",
        "    dropout_prob = 0.5              # 드롭아웃의 확률: 과적합 방지를 위한 비활성화 확률\n",
        "    cnn_out_dim = 50                # CNN 출력 차원"
      ],
      "metadata": {
        "id": "3jd2PETApN42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================= Utils ================="
      ],
      "metadata": {
        "id": "ZB31m_dm4Swj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "4EdnpsU8QuCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def date(f='%Y-%m-%d %H:%M:%S'):\n",
        "    return time.strftime(f, time.localtime())\n",
        "\n",
        "# glove.6B.50d.txt 파일 load하고 embedding하는 함수\n",
        "def load_embedding(word2vec_file):\n",
        "    with open(word2vec_file, encoding='utf-8') as f:\n",
        "        word_emb = [[0]]\n",
        "        word_dict = {'<UNK>': 0}\n",
        "        for line in f.readlines():\n",
        "            tokens = line.split(' ')\n",
        "            word_emb.append([float(i) for i in tokens[1:]])\n",
        "            word_dict[tokens[0]] = len(word_dict)\n",
        "        word_emb[0] = [0] * len(word_emb[1])\n",
        "    return word_emb, word_dict\n",
        "\n",
        "\n",
        "# MSE 계산하는 함수 -> train과 valid에서 사용\n",
        "def predict_mse(model, dataloader, device):\n",
        "    mse, sample_count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            user_reviews, item_reviews, ratings = map(lambda x: x.to(device), batch)\n",
        "            predict = model(user_reviews, item_reviews)\n",
        "            mse += F.mse_loss(predict, ratings, reduction='sum').item()\n",
        "            sample_count += len(ratings)\n",
        "    return mse / sample_count\n",
        "\n",
        "# Pytorch의 Dataset 클래스를 확장해서 DeepCoNN 모델의 학습에 적합한 데이터셋을 생성하는 Class\n",
        "class DeepCoNNDataset(Dataset):\n",
        "\n",
        "    ## csv 파일을 읽고 모델 입력 형식에 맞게 바꾸는 함수\n",
        "    def __init__(self, data_path, word_dict, config, retain_rui=True):\n",
        "        self.word_dict = word_dict\n",
        "        self.config = config\n",
        "        self.retain_rui = retain_rui                        # 리뷰 포함에 대한 조건\n",
        "        self.PAD_WORD_idx = self.word_dict[config.PAD_WORD]\n",
        "        self.review_length = config.review_length\n",
        "        self.review_count = config.review_count\n",
        "        self.lowest_r_count = config.lowest_review_count\n",
        "\n",
        "        df = pd.read_csv(data_path, header=None, names=['userID', 'itemID', 'review', 'rating'])\n",
        "        df['review'] = df['review'].apply(self._review2id)\n",
        "        self.sparse_idx = set()\n",
        "\n",
        "        user_reviews = self._get_reviews(df)\n",
        "        item_reviews = self._get_reviews(df, 'itemID', 'userID')\n",
        "        rating = torch.Tensor(df['rating'].to_list()).view(-1, 1)\n",
        "\n",
        "        # 희소 데이터(self.sparse_idx)를 제외하고 필터 -> lowest_review_count = 0이기에 손실되는 데이터 없을 것\n",
        "        self.user_reviews = user_reviews[[idx for idx in range(user_reviews.shape[0]) if idx not in self.sparse_idx]]\n",
        "        self.item_reviews = item_reviews[[idx for idx in range(item_reviews.shape[0]) if idx not in self.sparse_idx]]\n",
        "        self.rating = rating[[idx for idx in range(rating.shape[0]) if idx not in self.sparse_idx]]\n",
        "\n",
        "\n",
        "    ## 데이터셋에서 특정 idx로 데이터를 반환함 -> (user_reviews, item_reviews, rating)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.user_reviews[idx], self.item_reviews[idx], self.rating[idx]\n",
        "\n",
        "\n",
        "    ## 데이터셋 전체 길이 반환\n",
        "    def __len__(self):\n",
        "        return self.rating.shape[0]\n",
        "\n",
        "\n",
        "    ## 사용자 또는 아이템 단위로 리뷰 데이터를 그룹화하고, 최대 리뷰 개수(10개)에 따라 정리\n",
        "    def _get_reviews(self, df, lead='userID', costar='itemID'):\n",
        "        reviews_by_lead = dict(list(df[[costar, 'review']].groupby(df[lead])))\n",
        "        lead_reviews = []\n",
        "        for idx, (lead_id, costar_id) in enumerate(zip(df[lead], df[costar])):\n",
        "            df_data = reviews_by_lead[lead_id]\n",
        "\n",
        "\n",
        "            # self.retain_rui = True: 사용자가 작성한 모든 리뷰를 가져와\n",
        "            # true이기에 else가 실행되지않을 것 -> reviews = df_data['review'].to_list()가 이미 모든 리뷰를 포함하기 때문\n",
        "            reviews = df_data['review'].to_list() if self.retain_rui else df_data['review'][df_data[costar] != costar_id].to_list()\n",
        "\n",
        "\n",
        "            if len(reviews) < self.lowest_r_count:\n",
        "                self.sparse_idx.add(idx)\n",
        "            reviews = self._adjust_review_list(reviews, self.review_length, self.review_count)      # 개수와 길이 조정\n",
        "            lead_reviews.append(reviews)\n",
        "        return torch.LongTensor(lead_reviews)\n",
        "\n",
        "\n",
        "    ## 리뷰 데이터를 고정된 리뷰 수(review_count)와 리뷰 길이(review_length)로 조정\n",
        "    def _adjust_review_list(self, reviews, r_length, r_count):\n",
        "        reviews = reviews[:r_count] + [[self.PAD_WORD_idx] * r_length] * (r_count - len(reviews))\n",
        "        reviews = [r[:r_length] + [0] * (r_length - len(r)) for r in reviews]\n",
        "        return reviews\n",
        "\n",
        "    ## 리뷰 문자열을 단어 임베딩 인덱스 리스트로 변환\n",
        "    def _review2id(self, review):\n",
        "        if not isinstance(review, str):\n",
        "            return []\n",
        "        return [self.word_dict.get(word, self.PAD_WORD_idx) for word in review.split()]"
      ],
      "metadata": {
        "id": "6v_YdPA2qepY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================= Model ================="
      ],
      "metadata": {
        "id": "r5esLuqHBHLV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzfMA9Armo8q"
      },
      "outputs": [],
      "source": [
        "# User와 item 리뷰에서 feature 추출하는 역할\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, config, word_dim):\n",
        "        super(CNN, self).__init__()\n",
        "        self.kernel_count = config.kernel_count\n",
        "        self.review_count = config.review_count\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=word_dim, out_channels=config.kernel_count, kernel_size=config.kernel_size, padding=(config.kernel_size - 1) // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(1, config.review_length)),\n",
        "            nn.Dropout(p=config.dropout_prob))\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(config.kernel_count * config.review_count, config.cnn_out_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=config.dropout_prob))\n",
        "\n",
        "    # latent 생성하는 함수\n",
        "    def forward(self, vec):\n",
        "        latent = self.conv(vec.permute(0, 2, 1))\n",
        "        latent = self.linear(latent.reshape(-1, self.kernel_count * self.review_count))\n",
        "        return latent\n",
        "\n",
        "# 추출된 feature로 상호작용(interaction)해서 최종 예측 rating을 생성\n",
        "class FactorizationMachine(nn.Module):\n",
        "    def __init__(self, p, k):                               # latent matrix / p = 입력 벡터의 차원, k = 잠재 차원\n",
        "        super().__init__()\n",
        "        self.v = nn.Parameter(torch.rand(p, k) / 10)        # 초기화를 해당 식으로 할당\n",
        "        self.linear = nn.Linear(p, 1, bias=True)            # 1 = 출력 크기\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # latent 생성하는 함수\n",
        "    def forward(self, x):\n",
        "        linear_part = self.linear(x)                        # 선형관계 모델링\n",
        "        inter_part1 = torch.mm(x, self.v) ** 2              # 모든 특징간의 합성곱 -> 입력 데이터 x와 latent matrix(self.v)\n",
        "        inter_part2 = torch.mm(x ** 2, self.v ** 2)         # 개별 특징의 제곱으로 행렬 곱 전개 -> x^2와 v^2\n",
        "\n",
        "        # inter_part1은 전체 효과 / inter_part2는 개별 효과의 합산 -> 이 둘을 빼면? = 전체 효과에서 개별 효과를 뺀 값\n",
        "        # 여기에서 뺀 값의 의미 = 개별 효과로 설명되지 않는 부분 -> 상호작용 효과\n",
        "        pair_interactions = torch.sum(inter_part1 - inter_part2, dim=1, keepdim=True)       # 값의 차이로 상호작용 추출 + 값 변환\n",
        "        pair_interactions = self.dropout(pair_interactions)\n",
        "        return linear_part + 0.5 * pair_interactions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 전체 모델 구조 정의 + CNN과 FactorizationMachine(FM) 결합해서 예측\n",
        "class DeepCoNN(nn.Module):\n",
        "    def __init__(self, config, word_emb):\n",
        "        super(DeepCoNN, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(word_emb))\n",
        "        self.cnn_u = CNN(config, word_dim=self.embedding.embedding_dim)             # user 리뷰 처리\n",
        "        self.cnn_i = CNN(config, word_dim=self.embedding.embedding_dim)             # item 리뷰 처리\n",
        "        self.fm = FactorizationMachine(config.cnn_out_dim * 2, 10)                  # latent로 최종 rating 예측\n",
        "\n",
        "    # 사용자 리뷰(user_review)와 아이템 리뷰(item_review)를 입력받아 처리하는 함수\n",
        "    def forward(self, user_review, item_review):\n",
        "\n",
        "        # 리뷰 데이터를 배치(batch) 단위로 평탄화(reshape)하여 임베딩에 적합하도록 변환\n",
        "        new_batch_size = user_review.shape[0] * user_review.shape[1]\n",
        "        user_review = user_review.reshape(new_batch_size, -1)\n",
        "        item_review = item_review.reshape(new_batch_size, -1)\n",
        "\n",
        "        # 임베딩 벡터 변환\n",
        "        u_vec = self.embedding(user_review)\n",
        "        i_vec = self.embedding(item_review)\n",
        "\n",
        "        user_latent = self.cnn_u(u_vec)\n",
        "        item_latent = self.cnn_i(i_vec)\n",
        "\n",
        "        concat_latent = torch.cat((user_latent, item_latent), dim=1)        # latent 결합\n",
        "        return self.fm(concat_latent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================= Training ================="
      ],
      "metadata": {
        "id": "sdfLirOsG31e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Training =================\n",
        "def train(train_dataloader, valid_dataloader, model, config, model_path):\n",
        "    print(f'{date()}## Start the training!')\n",
        "    train_mse = predict_mse(model, train_dataloader, config.device)\n",
        "    valid_mse = predict_mse(model, valid_dataloader, config.device)\n",
        "    print(f'{date()}#### Initial train mse {train_mse:.6f}, validation mse {valid_mse:.6f}')\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # weight_decay = L2 정규화\n",
        "    opt = torch.optim.Adam(model.parameters(), config.learning_rate, weight_decay=config.l2_regularization)\n",
        "\n",
        "    # 학습률 스케줄\n",
        "    # ExponentialLR을 사용하여 학습률을 매 에포크마다 learning_rate_decay 비율로 줄이는 방식\n",
        "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(opt, config.learning_rate_decay)\n",
        "\n",
        "    best_loss = float('inf')    # 최소값을 위한 초기값 설정 -> validation에서의 최소 mse값\n",
        "\n",
        "    for epoch in range(config.train_epochs):\n",
        "        model.train()\n",
        "        total_loss, total_samples = 0, 0\n",
        "        for batch in train_dataloader:\n",
        "            user_reviews, item_reviews, ratings = map(lambda x: x.to(config.device), batch)\n",
        "            predict = model(user_reviews, item_reviews)\n",
        "\n",
        "            # F.mse_loss를 사용해 예측값과 실제값 간의 평균 제곱 오차(MSE)를 계산\n",
        "            loss = F.mse_loss(predict, ratings, reduction='sum')\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_samples += len(ratings)\n",
        "\n",
        "        lr_sch.step()\n",
        "        model.eval()\n",
        "        valid_mse = predict_mse(model, valid_dataloader, config.device)\n",
        "        train_loss = total_loss / total_samples\n",
        "        print(f\"{date()}#### Epoch {epoch:3d}; train mse {train_loss:.6f}; validation mse {valid_mse:.6f}\")\n",
        "\n",
        "        if best_loss > valid_mse:\n",
        "            best_loss = valid_mse\n",
        "            torch.save(model, model_path)\n",
        "\n",
        "    print(f'{date()}## End of training! Time used {time.perf_counter() - start_time:.0f} seconds.')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QTNriKZNrqkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================= Test, Main ================="
      ],
      "metadata": {
        "id": "3RWaFBzKG8RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# ================= Testing =================\n",
        "def test(dataloader, model, config):\n",
        "    print(f'{date()}## Start the testing!')\n",
        "    test_loss = predict_mse(model, dataloader, config.device)\n",
        "    print(f\"{date()}## Test end, test mse is {test_loss:.6f}\")\n",
        "\n",
        "\n",
        "# ================= Main =================\n",
        "if __name__ == '__main__':\n",
        "    config = Config()\n",
        "    print(config)\n",
        "    print(f'{date()}## Load embedding and data...')\n",
        "    word_emb, word_dict = load_embedding(config.word2vec_file)\n",
        "\n",
        "    train_dataset = DeepCoNNDataset(config.train_file, word_dict, config)\n",
        "    valid_dataset = DeepCoNNDataset(config.valid_file, word_dict, config, retain_rui=False)\n",
        "    test_dataset = DeepCoNNDataset(config.test_file, word_dict, config, retain_rui=False)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=config.batch_size)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
        "\n",
        "    # 모델 생성 및 학습\n",
        "    model = DeepCoNN(config, word_emb).to(config.device)\n",
        "    os.makedirs(os.path.dirname(config.model_file), exist_ok=True)\n",
        "    train(train_dataloader, valid_dataloader, model, config, config.model_file)\n",
        "\n",
        "    # 로드할 때 필요한 클래스들을 안전한 글로벌에 추가\n",
        "\n",
        "    torch.serialization.add_safe_globals([\n",
        "        DeepCoNN,\n",
        "        CNN,\n",
        "        FactorizationMachine,\n",
        "        torch.nn.modules.sparse.Embedding,\n",
        "        torch.nn.modules.container.Sequential,\n",
        "        torch.nn.modules.conv.Conv1d,\n",
        "        torch.nn.modules.activation.ReLU,\n",
        "        torch.nn.modules.pooling.MaxPool2d,\n",
        "        torch.nn.modules.dropout.Dropout,\n",
        "        torch.nn.modules.linear.Linear\n",
        "    ])\n",
        "    loaded_model = torch.load(config.model_file)\n",
        "\n",
        "    test(test_dataloader, loaded_model, config)\n",
        "\n",
        "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Valid dataset size: {len(valid_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    # 데이터 확인\n",
        "    for i, (user_review, item_review, rating) in enumerate(train_dataloader):\n",
        "        print(f\"Batch {i+1}:\")\n",
        "        print(f\"User Review Tensor: {user_review.shape}\")\n",
        "        print(f\"Item Review Tensor: {item_review.shape}\")\n",
        "        print(f\"Rating Tensor: {rating.shape}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASBkkEPSSz4V",
        "outputId": "4b895525-08be-4f2c-e145-10200922ca3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Config object at 0x7e2a0edad5d0>\n",
            "2025-03-29 09:55:14## Load embedding and data...\n",
            "2025-03-29 09:55:27## Start the training!\n",
            "2025-03-29 09:55:27#### Initial train mse 11.684132, validation mse 11.840652\n",
            "2025-03-29 09:55:28#### Epoch   0; train mse 5.387804; validation mse 6.064975\n",
            "2025-03-29 09:55:28#### Epoch   1; train mse 2.411958; validation mse 3.843110\n",
            "2025-03-29 09:55:29#### Epoch   2; train mse 2.061943; validation mse 3.581172\n",
            "2025-03-29 09:55:30#### Epoch   3; train mse 1.954420; validation mse 3.242029\n",
            "2025-03-29 09:55:31#### Epoch   4; train mse 1.862581; validation mse 3.120563\n",
            "2025-03-29 09:55:32#### Epoch   5; train mse 1.808903; validation mse 2.913261\n",
            "2025-03-29 09:55:32#### Epoch   6; train mse 1.749055; validation mse 2.979349\n",
            "2025-03-29 09:55:33#### Epoch   7; train mse 1.715458; validation mse 3.149965\n",
            "2025-03-29 09:55:33#### Epoch   8; train mse 1.687228; validation mse 3.096098\n",
            "2025-03-29 09:55:34#### Epoch   9; train mse 1.641868; validation mse 3.411059\n",
            "2025-03-29 09:55:35#### Epoch  10; train mse 1.607224; validation mse 2.989934\n",
            "2025-03-29 09:55:35#### Epoch  11; train mse 1.543505; validation mse 3.056639\n",
            "2025-03-29 09:55:36#### Epoch  12; train mse 1.524818; validation mse 2.965557\n",
            "2025-03-29 09:55:36#### Epoch  13; train mse 1.506795; validation mse 2.986355\n",
            "2025-03-29 09:55:37#### Epoch  14; train mse 1.500980; validation mse 3.014248\n",
            "2025-03-29 09:55:37#### Epoch  15; train mse 1.450206; validation mse 2.881462\n",
            "2025-03-29 09:55:38#### Epoch  16; train mse 1.452439; validation mse 3.095408\n",
            "2025-03-29 09:55:39#### Epoch  17; train mse 1.414658; validation mse 3.099782\n",
            "2025-03-29 09:55:39#### Epoch  18; train mse 1.380585; validation mse 2.884812\n",
            "2025-03-29 09:55:40#### Epoch  19; train mse 1.381577; validation mse 3.048735\n",
            "2025-03-29 09:55:40## End of training! Time used 13 seconds.\n",
            "2025-03-29 09:55:40## Start the testing!\n",
            "2025-03-29 09:55:40## Test end, test mse is 2.858286\n",
            "Train dataset size: 12790\n",
            "Valid dataset size: 1599\n",
            "Test dataset size: 1600\n",
            "Batch 1:\n",
            "User Review Tensor: torch.Size([128, 10, 40])\n",
            "Item Review Tensor: torch.Size([128, 10, 40])\n",
            "Rating Tensor: torch.Size([128, 1])\n"
          ]
        }
      ]
    }
  ]
}
