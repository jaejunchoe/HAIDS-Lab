{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "1KH1kvjFQ5a-"
      ],
      "authorship_tag": "ABX9TyPKgL8fkrk2ZQNxpQPiVlhj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaejunchoe/HAIDS-Lab/blob/main/Upload_ver02_D_attn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwtITRADQpDw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 준비준비"
      ],
      "metadata": {
        "id": "1KH1kvjFQ5a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1번 실행 -> 런타임 다시 시작 -> 2번 실행 -> 3번 실행\n",
        "\n",
        "## 1번\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q97Yc9hBQ6pc",
        "outputId": "93c1b8a4-f06e-46bb-f33d-7a42fddf3a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (7,840 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2번\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic')"
      ],
      "metadata": {
        "id": "iPDuj2z0Q7ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3번\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import seaborn as sns\n",
        "\n",
        "# 나눔고딕 폰트 경로 설정\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
        "plt.rc('font', family=font_name)\n",
        "\n",
        "# 그래프 그리기 예시\n",
        "data = {'가나다': [10, 15, 7], '라마바': [20, 8, 12]}\n",
        "df = pd.DataFrame(data)\n",
        "sns.barplot(data=df, x='가나다', y='라마바')\n",
        "plt.title('한글 그래프')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "prl3g6SAQ8Ws",
        "outputId": "8c49ca23-0f4e-470d-a8ae-0125a3a36bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시작"
      ],
      "metadata": {
        "id": "IF_xwQWCQ9-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdcftNTBQ_n0",
        "outputId": "31d9c16e-6271-46ea-eaa7-3ebff1f2ac4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data_loader"
      ],
      "metadata": {
        "id": "Fh3J2-gVR08O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset과 Embedding 파일을 읽고 preprocessing"
      ],
      "metadata": {
        "id": "LoUmS9jTAFUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from nltk import word_tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "\n",
        "# Dataset Definition -> Role: Data load, Review preprocessing, Transforming embedding\n",
        "class ReviewDataset(Dataset):\n",
        "\n",
        "    # 데이터에 대한 리뷰 길이 10,000으로 제한하고, embed_size 기반으로 padding, unknown, delimiter(구분자) 데이터 설정하는 함수\n",
        "    def __init__(self, csv_path, emb_path, max_len=10000, embed_size=64):\n",
        "        \"\"\"\n",
        "        데이터셋 클래스\n",
        "        :param csv_path: 전처리된 CSV 파일 경로\n",
        "        :param emb_path: 단어 임베딩 파일 경로\n",
        "        :param max_len: 리뷰 최대 길이\n",
        "        :param embed_size: 임베딩 차원\n",
        "        \"\"\"\n",
        "        self.dataset = pd.read_csv(csv_path, header=None, names=['userID', 'itemID', 'review', 'rating'])\n",
        "\n",
        "        ## 단어 임베딩 로드\n",
        "        with open(emb_path, 'rb') as f:\n",
        "            self.word_emb = pickle.load(f)\n",
        "\n",
        "        # 임베딩 벡터 설정\n",
        "        self.pad = np.zeros(embed_size)\n",
        "        self.unknown = np.random.uniform(0, 1, embed_size)\n",
        "        self.delimiter = np.random.uniform(0, 1, embed_size)\n",
        "\n",
        "        # 하이퍼파라미터\n",
        "        self.max_len = max_len\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "\n",
        "    # user와 item review에 대해 preprocessing + preprocessing된 review와 rating을 반환하는 함수\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        하나의 데이터 샘플을 가져옵니다.\n",
        "        \"\"\"\n",
        "        row = self.dataset.loc[index]\n",
        "        user_id = row['userID']\n",
        "        item_id = row['itemID']\n",
        "\n",
        "        # 리뷰 데이터\n",
        "        user_review = self.preprocess_review(user_id, \"User\")\n",
        "        item_review = self.preprocess_review(item_id, \"Item\")\n",
        "\n",
        "        # 평점\n",
        "        rating = torch.tensor(row['rating'], dtype=torch.float)\n",
        "\n",
        "        return torch.tensor(user_review, dtype=torch.float), \\\n",
        "               torch.tensor(item_review, dtype=torch.float), \\\n",
        "               rating\n",
        "\n",
        "\n",
        "    # 데이터셋의 총 길이(샘플 개수)를 반환 -> 데이터셋의 크기 확인\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "\n",
        "    # user 또는 item id에 맞는 review를 읽고 토큰화한 뒤 word_emb에서 embedding vector로 변환하고 max_len에 맞추어 처리하는 함수\n",
        "    def preprocess_review(self, entity_id, entity_type):\n",
        "        \"\"\"\n",
        "        리뷰 파일을 로드하고 임베딩을 적용합니다.\n",
        "        \"\"\"\n",
        "        file_path = f\"data/{entity_type}/{entity_id}.tsv\"                   # entity_type: user or item에 대한 문자열로 user와 item을 구분\n",
        "                                                                            # entity_id: 해당 user의 id or item의 id\n",
        "                                                                            # 해당 부분들은 동적으로 생성되는 구조 = file_path\n",
        "\n",
        "\n",
        "\n",
        "        try:\n",
        "            reviews = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "        except Exception:\n",
        "            return [self.pad] * self.max_len                                # 만약 파일이 존재하지 않다면, [self.pad] * self.max_len로 반환.\n",
        "\n",
        "        total_review = []\n",
        "        for review_str in reviews[0][:100]:  # 최대 100개의 리뷰 단어\n",
        "            tokens = word_tokenize(review_str)\n",
        "            for word in tokens:\n",
        "                if word in self.word_emb:\n",
        "                    total_review.append(self.word_emb[word])                # word_emb에 있는 단어에는 embedding vector로 변환\n",
        "                else:\n",
        "                    total_review.append(self.unknown)                       # word_emb에서 없는 단어의 경우 unknown vector로 사용\n",
        "            total_review.append(self.delimiter)                             # 각 review 문장 끝날 때마다 구분 벡터로 리뷰 간의 경계선 역할을 진행\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ## 리뷰 길이 조정 (패딩 또는 자르기) -> max_len(10,000)보다 길면 자르고 짧으면 pad로 padding\n",
        "        if len(total_review) < self.max_len:\n",
        "            pad_len = self.max_len - len(total_review)\n",
        "            total_review += [self.pad] * pad_len\n",
        "        else:\n",
        "            total_review = total_review[:self.max_len]\n",
        "\n",
        "        return np.array(total_review)                                       # numpy 배열로 변환하여 반환하고 이 결과는 user or item의 review를 vector화한 max_len(10,000)의 길이 배열\n",
        "\n",
        "\n",
        "# Batch 생성 시 None 데이터를 제거 -> DataLoader에서 batch를 만들 때, None 값이 포함된 데이터는 제거하는 함수\n",
        "def my_collate(batch):\n",
        "    \"\"\"\n",
        "    None 데이터를 필터링하는 Collate 함수.\n",
        "    \"\"\"\n",
        "    batch = list(filter(lambda x: x is not None, batch))                    # batch = DataLoader로부터 전달된 데이터 리스트\n",
        "    return default_collate(batch)                                           # default_collate: pytorch에서 제공하는 배치 생성 함수로, tensor 형태로 변환\n",
        "\n",
        "\n",
        "\n",
        "# DataLoader를 생성하는 Pytorch 함수 -> ReviewDataset 클래스를 기반으로 batch 단위의 DataLoader를 반환하는 역할\n",
        "def get_loader(csv_path, emb_path, batch_size=32, shuffle=True, num_workers=2):\n",
        "    \"\"\"\n",
        "    데이터 로더 생성 함수.\n",
        "    :param csv_path: 전처리된 CSV 파일 경로\n",
        "    :param emb_path: 단어 임베딩 파일 경로\n",
        "    \"\"\"\n",
        "    dataset = ReviewDataset(csv_path, emb_path)                             # ReviewDataset 객체를 생성해 데이터셋을 초기화함.\n",
        "    data_loader = DataLoader(dataset=dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=my_collate)                         # collate_fn: 배치 생성 시 사용할 함수이며, def my_collate를 사용한다.\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "# 파일 경로 설정\n",
        "train_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2_train.csv'\n",
        "valid_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2__valid.csv'\n",
        "test_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/dataset/cleaned_Transnet_T2_test.csv'\n",
        "emb_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/combined_word_emb.pkl'\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_loader = get_loader(train_path, emb_path, batch_size=32, shuffle=True)\n",
        "valid_loader = get_loader(valid_path, emb_path, batch_size=32, shuffle=False)\n",
        "test_loader = get_loader(test_path, emb_path, batch_size=32, shuffle=False)\n",
        "\n",
        "# 데이터 확인\n",
        "for i, (user_review, item_review, rating) in enumerate(train_loader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"User Review Tensor: {user_review.shape}\")\n",
        "    print(f\"Item Review Tensor: {item_review.shape}\")\n",
        "    print(f\"Rating Tensor: {rating.shape}\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "zwBmtLhnR1H1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818fc93e-8f0f-4114-ff36-94f4266ca513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "User Review Tensor: torch.Size([32, 10000, 64])\n",
            "Item Review Tensor: torch.Size([32, 10000, 64])\n",
            "Rating Tensor: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 너무 느리다면? 아래의 코드로 수정\n",
        "\n",
        "\n",
        "\n",
        "def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    하나의 데이터 샘플을 가져옵니다.\n",
        "    \"\"\"\n",
        "    row = self.dataset.loc[index]\n",
        "    user_id = row['userID']\n",
        "    item_id = row['itemID']\n",
        "    \n",
        "    # 리뷰 데이터\n",
        "    user_review = np.array(self.preprocess_review(user_id, \"User\"))  # numpy 변환 추가\n",
        "    item_review = np.array(self.preprocess_review(item_id, \"Item\"))  # numpy 변환 추가\n",
        "    \n",
        "    # 평점\n",
        "    rating = torch.tensor(row['rating'], dtype=torch.float)\n",
        "    \n",
        "    return torch.tensor(user_review, dtype=torch.float), \\\n",
        "           torch.tensor(item_review, dtype=torch.float), \\\n",
        "           rating\n"
      ],
      "metadata": {
        "id": "uDY-66P6Wm_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture"
      ],
      "metadata": {
        "id": "55zVZHr7lPN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Local vs Global**\n",
        "\n",
        "\\\n",
        "\n",
        "Local: text(review) 내에서 국소적(지역적) 관점에서 중요한 키워드를 선택하는데에 집중하는 성향.\n",
        "\n",
        "즉, 특정 키워드 분석하듯이 개별 단어 수준의 세부적 특성을 파악한다. \\\n",
        "\n",
        "\\\n",
        "\n",
        "Global: text(review)의 전체적인 의미를 반영해 text(review)의 전역적인 문맥을 파악하는 성향.\n",
        "\n",
        "즉, text(review)의 전반적인 의미를 포착하는데 사용되며, 긴 리뷰에서의 noise 제거와 핵심 단어 추출에 좀 더 용이하다.\n"
      ],
      "metadata": {
        "id": "GT9r-vEtF_Cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예시: 리뷰 문장** \\\n",
        "\"The service was excellent, and the atmosphere was cozy, but the food was overpriced and disappointing.\"\n",
        "\n",
        "\\\\\n",
        "\n",
        "**If) Local_Attention** \\\n",
        "\"The service was excellent\"라는 부분에 대한 attention은 \"service\"와 \"excellent\"라는 단어에 집중하고, \"was\"나 다른 부분은 상대적으로 덜 주목하게 됨. \\\n",
        "\"the atmosphere was cozy\"는 \"atmosphere\"와 \"cozy\"에 초점을 맞춤 \\\n",
        "\"the food was overpriced and disappointing\"는 \"food\", \"overpriced\", \"disappointing\" 간의 관계에 주목. \\\n",
        "\n",
        " \\\n",
        " \\\n",
        "\n",
        "**If) Golbal_Attention** \\\n",
        "\n",
        "\"excellent\", \"cozy\", \"overpriced\", \"disappointing\" 같은 감정 표현들이 문장 전체에서 서로 어떻게 연결되는지 이해하기 위해 분석함. \\\n",
        "\n",
        "-> 문장의 전반적인 톤과 주제를 판단하는 데 적합한 형태.\n",
        "\n"
      ],
      "metadata": {
        "id": "8f0euazBGAnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "bLLoVgw0lcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Local 문맥을 학습하고 단어 간 가까운 상호작용 정보를 추출.\n",
        "# Local 문맥에서 단어 간 상호작용을 학습하기 위해 설계된 Attention 레이어.\n",
        "class LocalAttention(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, win_size, out_channels):\n",
        "        super(LocalAttention, self).__init__()                                                                     # 'win_size = 5'를 통해 단어 주변의 문맥 정의.\n",
        "\n",
        "        self.win_size = win_size\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 1, kernel_size=(win_size, embed_size)),                                                   # CNN 기반으로 local 문맥의 중요도를 변환하는 Attention_layer\n",
        "            nn.Sigmoid())                                                                                          # 'win_size = 5'를 고려하여 각 단어의 중요도를 0~1로 표현 -> keypoint: win_size = 5\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(1, embed_size)),                                               # Local 문맥을 처리하는 CNN Layer\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size, 1)))                                                                         # 문맥 정보를 집약해 최종적으로 input_size인 10,000으로 Feature map 생성\n",
        "\n",
        "\n",
        "    ## input data를 padding하고 Attention layer 및 CNN에 전달하여 처리된 output 반환.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Padding dynamically to ensure compatibility\n",
        "        #padding = torch.zeros(x.size(0), (self.win_size - 1) // 2, x.size(-1))\n",
        "        padding = torch.zeros(x.size(0), (self.win_size - 1) // 2, x.size(-1), device=x.device)                    # Padding 처리.\n",
        "        x_pad = torch.cat((padding, x, padding), 1).unsqueeze(1)\n",
        "\n",
        "        scores = self.attention_layer(x_pad).squeeze(1)                                                            # Attention score 계산: 중요도 계산을 통한 단어별 가중치 적용\n",
        "        out = torch.mul(x, scores).unsqueeze(1)\n",
        "        out = self.cnn(out)                                                                                        # 최종적으로 Local 문맥 정보 변환\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Global 문맥을 학습하고 다양한 길이의 단어 간 상호작용 정보를 추출.\n",
        "# Global 문맥에서 단어 간 상호작용을 학습하기 위한 Attention 레이어.\n",
        "class GlobalAttention(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, out_channels):\n",
        "        super(GlobalAttention, self).__init__()\n",
        "\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 1, kernel_size=(input_size, embed_size)),                                                  # Global 단위의 중요도를 계산하는 CNN 기반의 Attention Layer\n",
        "            nn.Sigmoid())                                                                                           # 입력 데이터의 길이(input_size)를 고려하여 각 단어의 중요도를 0~1로 표현 -> keypoint: 입력 데이터의 길이(input_size)\n",
        "\n",
        "\n",
        "        ## cnn_1,2,3: Global 문맥의 처리를 위한 CNN Layer\n",
        "        ## 나머지 구성은 동일함.\n",
        "\n",
        "\n",
        "        self.cnn_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(2, embed_size)),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size - 2 + 1, 1)))\n",
        "        self.cnn_2 = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(3, embed_size)),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size - 3 + 1, 1)))\n",
        "        self.cnn_3 = nn.Sequential(\n",
        "            nn.Conv2d(1, out_channels, kernel_size=(4, embed_size)),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((input_size - 4 + 1, 1)))\n",
        "\n",
        "\n",
        "    # CNN 필터 크기별로 Global 문맥을 처리하는 함수\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        score = self.attention_layer(x)\n",
        "        out = torch.mul(x, score)\n",
        "        return self.cnn_1(out), self.cnn_2(out), self.cnn_3(out)\n",
        "\n",
        "\n",
        "\n",
        "# 사용자와 아이템 리뷰 데이터를 처리하고, 로컬 및 전역 Attention 메커니즘을 적용한 후 예측 값을 생성하는 메인 모델.\n",
        "# User와 Item review를 기반으로 Local과 Global 문맥을 결합하고 최종 Rating을 예측.\n",
        "\n",
        "class CNNDLGA(nn.Module):\n",
        "    def __init__(self, input_size, embed_size=100, win_size=5, channels_local=200, channels_global=100,\n",
        "                 hidden_size=500, output_size=50):\n",
        "        super(CNNDLGA, self).__init__()\n",
        "\n",
        "        self.localAttentionLayer_user = LocalAttention(input_size, embed_size, win_size, channels_local)\n",
        "        self.globalAttentionLayer_user = GlobalAttention(input_size, embed_size, channels_global)\n",
        "        self.localAttentionLayer_item = LocalAttention(input_size, embed_size, win_size, channels_local)\n",
        "        self.globalAttentionLayer_item = GlobalAttention(input_size, embed_size, channels_global)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        # CNN에서 생성된 Feature map을 고차원 벡터로 변환하는 역할.\n",
        "        self.fcLayer = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),  # 입력 차원을 명시하지 않음\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    # LocalAttentionLayer_user와 GlobalAttentionLayer_user를 사용해 문맥 정보를 추출.\n",
        "    def forward(self, x_user, x_item):\n",
        "        # User side\n",
        "        local_user = self.localAttentionLayer_user(x_user)\n",
        "        global1_user, global2_user, global3_user = self.globalAttentionLayer_user(x_user)\n",
        "        out_user = torch.cat((local_user, global1_user, global2_user, global3_user), 1).view(x_user.size(0), -1)                # CNN 출력을 연결(torch.cat)하여 Fully Conntected Layer로 전달.\n",
        "        out_user = self.fcLayer(out_user)\n",
        "\n",
        "        # Item side\n",
        "        local_item = self.localAttentionLayer_item(x_item)\n",
        "        global1_item, global2_item, global3_item = self.globalAttentionLayer_item(x_item)\n",
        "        out_item = torch.cat((local_item, global1_item, global2_item, global3_item), 1).view(x_item.size(0), -1)                # CNN 출력을 연결(torch.cat)하여 Fully Conntected Layer로 전달.\n",
        "        out_item = self.fcLayer(out_item)\n",
        "\n",
        "        # Combine user and item representations\n",
        "        out = torch.sum(torch.mul(out_user, out_item), 1)                                                                       # 두 출력 벡터를 곱(torch.mul)한 후 합산(torch.sum)하여 최종 rating 예측.\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "P9l84Y7bWnKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gpu 사용"
      ],
      "metadata": {
        "id": "DdbouJRKvo_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())  # True면 GPU 사용 가능\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfUYYx4cvqCL",
        "outputId": "00daa7bd-515d-4d00-af5b-d0c63cb79156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Result 1"
      ],
      "metadata": {
        "id": "iZvpr8RtOC6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# ================= Hyperparameters =================\n",
        "input_size = 10000        # 리뷰 길이\n",
        "embed_size = 64           # 임베딩 차원\n",
        "num_epochs = 10           # 학습 에폭 수\n",
        "batch_size = 32           # 배치 사이즈\n",
        "learning_rate = 1e-4      # 학습률\n",
        "hidden_size = 500         # FC 레이어 히든 사이즈\n",
        "output_size = 1           # 출력 사이즈 (평점 예측)\n",
        "win_size = 5              # 윈도우 크기\n",
        "channels_local = 200      # LocalAttention 채널 수\n",
        "channels_global = 100     # GlobalAttention 채널 수\n",
        "save_path = '/content/drive/MyDrive/IDS/amaxon reviews 2023/D-attn/model save2/'  # Google Drive 저장 경로 -> validation에서의 최적 파라미터 정보가 담긴 pickle 파일을 저장하는 경로\n",
        "\n",
        "# ================= Utility Functions =================\n",
        "\n",
        "# def evaluation은 필요 X -> MSE 값으로 하는 걸로 변경\n",
        "def evaluation(target, cf_out):\n",
        "    \"\"\"AUC 계산\"\"\"\n",
        "    fpr, tpr, _ = metrics.roc_curve(target, cf_out)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "    return auc\n",
        "\n",
        "def to_var(x):\n",
        "    \"\"\"CUDA 변환 함수\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x)\n",
        "\n",
        "# ================= Loading Data =================\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = get_loader(train_path, emb_path, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = get_loader(valid_path, emb_path, batch_size=batch_size, shuffle=False)\n",
        "test_loader = get_loader(test_path, emb_path, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"train/valid/test: {len(train_loader)}/{len(valid_loader)}/{len(test_loader)}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQIM2SS_ODWx",
        "outputId": "bd68e131-bcd5-482b-eff2-131a8b882d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "train/valid/test: 400/50/50\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Model Initialization =================\n",
        "model = CNNDLGA(input_size=input_size, embed_size=embed_size, win_size=win_size,\n",
        "                channels_local=channels_local, channels_global=channels_global,\n",
        "                hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "# 손실 함수 및 최적화기\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VfHs9xs4OUxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Training Start..\")\n",
        "\n",
        "best_mse = float(\"inf\") # 최소값을 위한 초기값 설정 -> validation에서의 최소 mse값\n",
        "\n",
        "# ================= Training Loop =================\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    batch_loss = 0.0\n",
        "\n",
        "    for i, (user_review, item_review, labels) in enumerate(train_loader):\n",
        "        # 데이터 CUDA 변환\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "        labels = to_var(labels)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(user_review, item_review)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and Optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], \"\n",
        "                  f\"Loss: {batch_loss / 10:.4f}\")\n",
        "            batch_loss = 0.0\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(model.state_dict(), f\"{save_path}model_epoch_{epoch+1}.pkl\")\n",
        "    print(f\"Model for epoch {epoch+1} saved.\")\n",
        "\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Training End..\")\n",
        "\n",
        "# ================= Validation Loop =================\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Validation Start...\")\n",
        "    model.eval()\n",
        "    all_outputs = []    # output_list\n",
        "    all_labels = []    # target_list\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_review, item_review, labels in valid_loader:\n",
        "            user_review = to_var(user_review)\n",
        "            item_review = to_var(item_review)\n",
        "            labels = to_var(labels)\n",
        "\n",
        "            outputs = model(user_review, item_review)\n",
        "\n",
        "            all_outputs.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 리스트 -> 텐서 변환\n",
        "    all_outputs = torch.tensor(all_outputs)  # 리스트를 PyTorch 텐서로 변환\n",
        "    all_labels = torch.tensor(all_labels)    # 리스트를 PyTorch 텐서로 변환\n",
        "\n",
        "    # MSE 계산\n",
        "    mse = torch.mean((all_labels - all_outputs) ** 2).item()\n",
        "    print(f\"Validation MSE: {mse:.4f}\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Validation End..\")\n",
        "\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        torch.save(model.state_dict(), f\"{save_path}best_model.pkl\")\n",
        "        print(f\"Best model for epoch {epoch+1} saved.\")"
      ],
      "metadata": {
        "id": "t7uX861mWBVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee08633-e4d0-4542-f64c-373e055c9010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Training Start..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/400], Loss: 16.6876\n",
            "Epoch [1/10], Step [20/400], Loss: 15.6218\n",
            "Epoch [1/10], Step [30/400], Loss: 15.8189\n",
            "Epoch [1/10], Step [40/400], Loss: 15.7029\n",
            "Epoch [1/10], Step [50/400], Loss: 13.9068\n",
            "Epoch [1/10], Step [60/400], Loss: 13.0283\n",
            "Epoch [1/10], Step [70/400], Loss: 9.9003\n",
            "Epoch [1/10], Step [80/400], Loss: 7.6002\n",
            "Epoch [1/10], Step [90/400], Loss: 4.7122\n",
            "Epoch [1/10], Step [100/400], Loss: 2.9395\n",
            "Epoch [1/10], Step [110/400], Loss: 2.7001\n",
            "Epoch [1/10], Step [120/400], Loss: 2.7571\n",
            "Epoch [1/10], Step [130/400], Loss: 2.6380\n",
            "Epoch [1/10], Step [140/400], Loss: 2.6610\n",
            "Epoch [1/10], Step [150/400], Loss: 2.5130\n",
            "Epoch [1/10], Step [160/400], Loss: 2.6326\n",
            "Epoch [1/10], Step [170/400], Loss: 2.5812\n",
            "Epoch [1/10], Step [180/400], Loss: 2.6127\n",
            "Epoch [1/10], Step [190/400], Loss: 2.4999\n",
            "Epoch [1/10], Step [200/400], Loss: 2.6779\n",
            "Epoch [1/10], Step [210/400], Loss: 2.6041\n",
            "Epoch [1/10], Step [220/400], Loss: 2.8134\n",
            "Epoch [1/10], Step [230/400], Loss: 2.5089\n",
            "Epoch [1/10], Step [240/400], Loss: 2.7114\n",
            "Epoch [1/10], Step [250/400], Loss: 2.5650\n",
            "Epoch [1/10], Step [260/400], Loss: 2.6721\n",
            "Epoch [1/10], Step [270/400], Loss: 2.4066\n",
            "Epoch [1/10], Step [280/400], Loss: 2.2733\n",
            "Epoch [1/10], Step [290/400], Loss: 2.5264\n",
            "Epoch [1/10], Step [300/400], Loss: 2.6784\n",
            "Epoch [1/10], Step [310/400], Loss: 2.4587\n",
            "Epoch [1/10], Step [320/400], Loss: 2.5531\n",
            "Epoch [1/10], Step [330/400], Loss: 2.4965\n",
            "Epoch [1/10], Step [340/400], Loss: 2.3567\n",
            "Epoch [1/10], Step [350/400], Loss: 2.5404\n",
            "Epoch [1/10], Step [360/400], Loss: 2.6963\n",
            "Epoch [1/10], Step [370/400], Loss: 2.7197\n",
            "Epoch [1/10], Step [380/400], Loss: 2.7625\n",
            "Epoch [1/10], Step [390/400], Loss: 2.4308\n",
            "Epoch [1/10], Step [400/400], Loss: 2.6050\n",
            "Model for epoch 1 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4579\n",
            "================================================================================\n",
            "Validation End..\n",
            "Best model for epoch 1 saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Step [10/400], Loss: 2.5365\n",
            "Epoch [2/10], Step [20/400], Loss: 2.6954\n",
            "Epoch [2/10], Step [30/400], Loss: 2.4770\n",
            "Epoch [2/10], Step [40/400], Loss: 2.3697\n",
            "Epoch [2/10], Step [50/400], Loss: 2.3778\n",
            "Epoch [2/10], Step [60/400], Loss: 2.5104\n",
            "Epoch [2/10], Step [70/400], Loss: 2.6006\n",
            "Epoch [2/10], Step [80/400], Loss: 2.5869\n",
            "Epoch [2/10], Step [90/400], Loss: 2.6845\n",
            "Epoch [2/10], Step [100/400], Loss: 2.6641\n",
            "Epoch [2/10], Step [110/400], Loss: 2.5094\n",
            "Epoch [2/10], Step [120/400], Loss: 2.5042\n",
            "Epoch [2/10], Step [130/400], Loss: 2.7223\n",
            "Epoch [2/10], Step [140/400], Loss: 2.4115\n",
            "Epoch [2/10], Step [150/400], Loss: 2.5762\n",
            "Epoch [2/10], Step [160/400], Loss: 2.5388\n",
            "Epoch [2/10], Step [170/400], Loss: 2.2448\n",
            "Epoch [2/10], Step [180/400], Loss: 2.7291\n",
            "Epoch [2/10], Step [190/400], Loss: 2.4772\n",
            "Epoch [2/10], Step [200/400], Loss: 2.6334\n",
            "Epoch [2/10], Step [210/400], Loss: 2.7197\n",
            "Epoch [2/10], Step [220/400], Loss: 2.4261\n",
            "Epoch [2/10], Step [230/400], Loss: 2.5509\n",
            "Epoch [2/10], Step [240/400], Loss: 2.6014\n",
            "Epoch [2/10], Step [250/400], Loss: 2.4933\n",
            "Epoch [2/10], Step [260/400], Loss: 2.5205\n",
            "Epoch [2/10], Step [270/400], Loss: 2.5791\n",
            "Epoch [2/10], Step [280/400], Loss: 2.6855\n",
            "Epoch [2/10], Step [290/400], Loss: 2.5467\n",
            "Epoch [2/10], Step [300/400], Loss: 2.4526\n",
            "Epoch [2/10], Step [310/400], Loss: 2.6697\n",
            "Epoch [2/10], Step [320/400], Loss: 2.3537\n",
            "Epoch [2/10], Step [330/400], Loss: 2.4356\n",
            "Epoch [2/10], Step [340/400], Loss: 2.6329\n",
            "Epoch [2/10], Step [350/400], Loss: 2.5364\n",
            "Epoch [2/10], Step [360/400], Loss: 2.4493\n",
            "Epoch [2/10], Step [370/400], Loss: 2.6746\n",
            "Epoch [2/10], Step [380/400], Loss: 2.4824\n",
            "Epoch [2/10], Step [390/400], Loss: 2.4759\n",
            "Epoch [2/10], Step [400/400], Loss: 2.5380\n",
            "Model for epoch 2 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4713\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Step [10/400], Loss: 2.2978\n",
            "Epoch [3/10], Step [20/400], Loss: 2.7257\n",
            "Epoch [3/10], Step [30/400], Loss: 2.8438\n",
            "Epoch [3/10], Step [40/400], Loss: 2.4132\n",
            "Epoch [3/10], Step [50/400], Loss: 2.5511\n",
            "Epoch [3/10], Step [60/400], Loss: 2.3767\n",
            "Epoch [3/10], Step [70/400], Loss: 2.8524\n",
            "Epoch [3/10], Step [80/400], Loss: 2.9100\n",
            "Epoch [3/10], Step [90/400], Loss: 2.7605\n",
            "Epoch [3/10], Step [100/400], Loss: 2.6890\n",
            "Epoch [3/10], Step [110/400], Loss: 2.4702\n",
            "Epoch [3/10], Step [120/400], Loss: 2.5535\n",
            "Epoch [3/10], Step [130/400], Loss: 2.7137\n",
            "Epoch [3/10], Step [140/400], Loss: 2.6145\n",
            "Epoch [3/10], Step [150/400], Loss: 2.7481\n",
            "Epoch [3/10], Step [160/400], Loss: 2.4238\n",
            "Epoch [3/10], Step [170/400], Loss: 2.3663\n",
            "Epoch [3/10], Step [180/400], Loss: 2.4301\n",
            "Epoch [3/10], Step [190/400], Loss: 2.6688\n",
            "Epoch [3/10], Step [200/400], Loss: 2.5420\n",
            "Epoch [3/10], Step [210/400], Loss: 2.3991\n",
            "Epoch [3/10], Step [220/400], Loss: 2.4722\n",
            "Epoch [3/10], Step [230/400], Loss: 2.3786\n",
            "Epoch [3/10], Step [240/400], Loss: 2.5810\n",
            "Epoch [3/10], Step [250/400], Loss: 2.6534\n",
            "Epoch [3/10], Step [260/400], Loss: 2.5860\n",
            "Epoch [3/10], Step [270/400], Loss: 2.6521\n",
            "Epoch [3/10], Step [280/400], Loss: 2.3648\n",
            "Epoch [3/10], Step [290/400], Loss: 2.3476\n",
            "Epoch [3/10], Step [300/400], Loss: 2.5447\n",
            "Epoch [3/10], Step [310/400], Loss: 2.5116\n",
            "Epoch [3/10], Step [320/400], Loss: 2.5583\n",
            "Epoch [3/10], Step [330/400], Loss: 2.9046\n",
            "Epoch [3/10], Step [340/400], Loss: 2.4179\n",
            "Epoch [3/10], Step [350/400], Loss: 2.4728\n",
            "Epoch [3/10], Step [360/400], Loss: 2.5079\n",
            "Epoch [3/10], Step [370/400], Loss: 2.6268\n",
            "Epoch [3/10], Step [380/400], Loss: 2.5895\n",
            "Epoch [3/10], Step [390/400], Loss: 2.6013\n",
            "Epoch [3/10], Step [400/400], Loss: 2.6373\n",
            "Model for epoch 3 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4599\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Step [10/400], Loss: 2.4701\n",
            "Epoch [4/10], Step [20/400], Loss: 2.6956\n",
            "Epoch [4/10], Step [30/400], Loss: 2.4705\n",
            "Epoch [4/10], Step [40/400], Loss: 2.6480\n",
            "Epoch [4/10], Step [50/400], Loss: 2.6452\n",
            "Epoch [4/10], Step [60/400], Loss: 2.4871\n",
            "Epoch [4/10], Step [70/400], Loss: 2.7743\n",
            "Epoch [4/10], Step [80/400], Loss: 2.4344\n",
            "Epoch [4/10], Step [90/400], Loss: 2.6534\n",
            "Epoch [4/10], Step [100/400], Loss: 2.4338\n",
            "Epoch [4/10], Step [110/400], Loss: 2.7076\n",
            "Epoch [4/10], Step [120/400], Loss: 2.6577\n",
            "Epoch [4/10], Step [130/400], Loss: 2.5741\n",
            "Epoch [4/10], Step [140/400], Loss: 2.5884\n",
            "Epoch [4/10], Step [150/400], Loss: 2.5703\n",
            "Epoch [4/10], Step [160/400], Loss: 2.3941\n",
            "Epoch [4/10], Step [170/400], Loss: 2.3689\n",
            "Epoch [4/10], Step [180/400], Loss: 2.3873\n",
            "Epoch [4/10], Step [190/400], Loss: 2.4073\n",
            "Epoch [4/10], Step [200/400], Loss: 2.7507\n",
            "Epoch [4/10], Step [210/400], Loss: 2.5953\n",
            "Epoch [4/10], Step [220/400], Loss: 2.6539\n",
            "Epoch [4/10], Step [230/400], Loss: 2.7166\n",
            "Epoch [4/10], Step [240/400], Loss: 2.6611\n",
            "Epoch [4/10], Step [250/400], Loss: 2.5572\n",
            "Epoch [4/10], Step [260/400], Loss: 2.7083\n",
            "Epoch [4/10], Step [270/400], Loss: 2.6566\n",
            "Epoch [4/10], Step [280/400], Loss: 2.3845\n",
            "Epoch [4/10], Step [290/400], Loss: 2.4232\n",
            "Epoch [4/10], Step [300/400], Loss: 2.7004\n",
            "Epoch [4/10], Step [310/400], Loss: 2.6275\n",
            "Epoch [4/10], Step [320/400], Loss: 2.4508\n",
            "Epoch [4/10], Step [330/400], Loss: 2.4398\n",
            "Epoch [4/10], Step [340/400], Loss: 2.7925\n",
            "Epoch [4/10], Step [350/400], Loss: 2.4272\n",
            "Epoch [4/10], Step [360/400], Loss: 2.3307\n",
            "Epoch [4/10], Step [370/400], Loss: 2.5031\n",
            "Epoch [4/10], Step [380/400], Loss: 2.5488\n",
            "Epoch [4/10], Step [390/400], Loss: 2.6135\n",
            "Epoch [4/10], Step [400/400], Loss: 2.5498\n",
            "Model for epoch 4 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4582\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Step [10/400], Loss: 2.4671\n",
            "Epoch [5/10], Step [20/400], Loss: 2.4916\n",
            "Epoch [5/10], Step [30/400], Loss: 2.6731\n",
            "Epoch [5/10], Step [40/400], Loss: 2.5409\n",
            "Epoch [5/10], Step [50/400], Loss: 2.6966\n",
            "Epoch [5/10], Step [60/400], Loss: 2.3325\n",
            "Epoch [5/10], Step [70/400], Loss: 2.7751\n",
            "Epoch [5/10], Step [80/400], Loss: 2.5966\n",
            "Epoch [5/10], Step [90/400], Loss: 2.7939\n",
            "Epoch [5/10], Step [100/400], Loss: 2.5126\n",
            "Epoch [5/10], Step [110/400], Loss: 2.5716\n",
            "Epoch [5/10], Step [120/400], Loss: 2.4509\n",
            "Epoch [5/10], Step [130/400], Loss: 2.3655\n",
            "Epoch [5/10], Step [140/400], Loss: 2.5272\n",
            "Epoch [5/10], Step [150/400], Loss: 2.6463\n",
            "Epoch [5/10], Step [160/400], Loss: 2.4608\n",
            "Epoch [5/10], Step [170/400], Loss: 2.5276\n",
            "Epoch [5/10], Step [180/400], Loss: 2.3418\n",
            "Epoch [5/10], Step [190/400], Loss: 2.4843\n",
            "Epoch [5/10], Step [200/400], Loss: 2.6347\n",
            "Epoch [5/10], Step [210/400], Loss: 2.7153\n",
            "Epoch [5/10], Step [220/400], Loss: 2.4900\n",
            "Epoch [5/10], Step [230/400], Loss: 2.2308\n",
            "Epoch [5/10], Step [240/400], Loss: 2.6732\n",
            "Epoch [5/10], Step [250/400], Loss: 2.4807\n",
            "Epoch [5/10], Step [260/400], Loss: 2.6589\n",
            "Epoch [5/10], Step [270/400], Loss: 2.4366\n",
            "Epoch [5/10], Step [280/400], Loss: 2.6259\n",
            "Epoch [5/10], Step [290/400], Loss: 2.5509\n",
            "Epoch [5/10], Step [300/400], Loss: 2.7334\n",
            "Epoch [5/10], Step [310/400], Loss: 2.5390\n",
            "Epoch [5/10], Step [320/400], Loss: 2.8258\n",
            "Epoch [5/10], Step [330/400], Loss: 2.6496\n",
            "Epoch [5/10], Step [340/400], Loss: 2.5053\n",
            "Epoch [5/10], Step [350/400], Loss: 2.3942\n",
            "Epoch [5/10], Step [360/400], Loss: 2.3941\n",
            "Epoch [5/10], Step [370/400], Loss: 2.7122\n",
            "Epoch [5/10], Step [380/400], Loss: 2.4782\n",
            "Epoch [5/10], Step [390/400], Loss: 2.3360\n",
            "Epoch [5/10], Step [400/400], Loss: 2.4844\n",
            "Model for epoch 5 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4754\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Step [10/400], Loss: 2.4748\n",
            "Epoch [6/10], Step [20/400], Loss: 2.3183\n",
            "Epoch [6/10], Step [30/400], Loss: 2.5965\n",
            "Epoch [6/10], Step [40/400], Loss: 2.4646\n",
            "Epoch [6/10], Step [50/400], Loss: 2.6809\n",
            "Epoch [6/10], Step [60/400], Loss: 2.6505\n",
            "Epoch [6/10], Step [70/400], Loss: 2.4944\n",
            "Epoch [6/10], Step [80/400], Loss: 2.5005\n",
            "Epoch [6/10], Step [90/400], Loss: 2.6079\n",
            "Epoch [6/10], Step [100/400], Loss: 2.7710\n",
            "Epoch [6/10], Step [110/400], Loss: 2.3417\n",
            "Epoch [6/10], Step [120/400], Loss: 2.3655\n",
            "Epoch [6/10], Step [130/400], Loss: 2.4203\n",
            "Epoch [6/10], Step [140/400], Loss: 2.2556\n",
            "Epoch [6/10], Step [150/400], Loss: 2.7107\n",
            "Epoch [6/10], Step [160/400], Loss: 2.6192\n",
            "Epoch [6/10], Step [170/400], Loss: 2.6011\n",
            "Epoch [6/10], Step [180/400], Loss: 2.7621\n",
            "Epoch [6/10], Step [190/400], Loss: 2.5929\n",
            "Epoch [6/10], Step [200/400], Loss: 2.7755\n",
            "Epoch [6/10], Step [210/400], Loss: 2.6766\n",
            "Epoch [6/10], Step [220/400], Loss: 2.4369\n",
            "Epoch [6/10], Step [230/400], Loss: 2.7065\n",
            "Epoch [6/10], Step [240/400], Loss: 2.6294\n",
            "Epoch [6/10], Step [250/400], Loss: 2.7513\n",
            "Epoch [6/10], Step [260/400], Loss: 2.6684\n",
            "Epoch [6/10], Step [270/400], Loss: 2.4558\n",
            "Epoch [6/10], Step [280/400], Loss: 2.8430\n",
            "Epoch [6/10], Step [290/400], Loss: 2.4900\n",
            "Epoch [6/10], Step [300/400], Loss: 2.6100\n",
            "Epoch [6/10], Step [310/400], Loss: 2.2809\n",
            "Epoch [6/10], Step [320/400], Loss: 2.6231\n",
            "Epoch [6/10], Step [330/400], Loss: 2.4469\n",
            "Epoch [6/10], Step [340/400], Loss: 2.4935\n",
            "Epoch [6/10], Step [350/400], Loss: 2.3691\n",
            "Epoch [6/10], Step [360/400], Loss: 2.4783\n",
            "Epoch [6/10], Step [370/400], Loss: 2.5702\n",
            "Epoch [6/10], Step [380/400], Loss: 2.4382\n",
            "Epoch [6/10], Step [390/400], Loss: 2.4636\n",
            "Epoch [6/10], Step [400/400], Loss: 2.5374\n",
            "Model for epoch 6 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4581\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Step [10/400], Loss: 2.6360\n",
            "Epoch [7/10], Step [20/400], Loss: 2.3543\n",
            "Epoch [7/10], Step [30/400], Loss: 2.6661\n",
            "Epoch [7/10], Step [40/400], Loss: 2.5767\n",
            "Epoch [7/10], Step [50/400], Loss: 2.6023\n",
            "Epoch [7/10], Step [60/400], Loss: 2.3155\n",
            "Epoch [7/10], Step [70/400], Loss: 2.7179\n",
            "Epoch [7/10], Step [80/400], Loss: 2.4004\n",
            "Epoch [7/10], Step [90/400], Loss: 2.3568\n",
            "Epoch [7/10], Step [100/400], Loss: 2.3297\n",
            "Epoch [7/10], Step [110/400], Loss: 2.4162\n",
            "Epoch [7/10], Step [120/400], Loss: 2.3110\n",
            "Epoch [7/10], Step [130/400], Loss: 2.5781\n",
            "Epoch [7/10], Step [140/400], Loss: 2.5540\n",
            "Epoch [7/10], Step [150/400], Loss: 2.4234\n",
            "Epoch [7/10], Step [160/400], Loss: 2.3210\n",
            "Epoch [7/10], Step [170/400], Loss: 2.6134\n",
            "Epoch [7/10], Step [180/400], Loss: 2.7054\n",
            "Epoch [7/10], Step [190/400], Loss: 2.5158\n",
            "Epoch [7/10], Step [200/400], Loss: 2.4378\n",
            "Epoch [7/10], Step [210/400], Loss: 2.4299\n",
            "Epoch [7/10], Step [220/400], Loss: 2.5346\n",
            "Epoch [7/10], Step [230/400], Loss: 2.5259\n",
            "Epoch [7/10], Step [240/400], Loss: 2.5259\n",
            "Epoch [7/10], Step [250/400], Loss: 2.4459\n",
            "Epoch [7/10], Step [260/400], Loss: 2.5155\n",
            "Epoch [7/10], Step [270/400], Loss: 2.5926\n",
            "Epoch [7/10], Step [280/400], Loss: 2.7960\n",
            "Epoch [7/10], Step [290/400], Loss: 2.7141\n",
            "Epoch [7/10], Step [300/400], Loss: 2.5425\n",
            "Epoch [7/10], Step [310/400], Loss: 2.4810\n",
            "Epoch [7/10], Step [320/400], Loss: 2.6061\n",
            "Epoch [7/10], Step [330/400], Loss: 2.9910\n",
            "Epoch [7/10], Step [340/400], Loss: 2.5631\n",
            "Epoch [7/10], Step [350/400], Loss: 2.6014\n",
            "Epoch [7/10], Step [360/400], Loss: 2.5650\n",
            "Epoch [7/10], Step [370/400], Loss: 2.3233\n",
            "Epoch [7/10], Step [380/400], Loss: 2.5448\n",
            "Epoch [7/10], Step [390/400], Loss: 2.6148\n",
            "Epoch [7/10], Step [400/400], Loss: 2.4219\n",
            "Model for epoch 7 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4581\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Step [10/400], Loss: 2.2770\n",
            "Epoch [8/10], Step [20/400], Loss: 2.4541\n",
            "Epoch [8/10], Step [30/400], Loss: 2.6130\n",
            "Epoch [8/10], Step [40/400], Loss: 2.7163\n",
            "Epoch [8/10], Step [50/400], Loss: 2.6873\n",
            "Epoch [8/10], Step [60/400], Loss: 2.6244\n",
            "Epoch [8/10], Step [70/400], Loss: 2.4043\n",
            "Epoch [8/10], Step [80/400], Loss: 2.4390\n",
            "Epoch [8/10], Step [90/400], Loss: 2.3553\n",
            "Epoch [8/10], Step [100/400], Loss: 2.7125\n",
            "Epoch [8/10], Step [110/400], Loss: 2.3139\n",
            "Epoch [8/10], Step [120/400], Loss: 2.6172\n",
            "Epoch [8/10], Step [130/400], Loss: 2.5179\n",
            "Epoch [8/10], Step [140/400], Loss: 2.4935\n",
            "Epoch [8/10], Step [150/400], Loss: 2.6653\n",
            "Epoch [8/10], Step [160/400], Loss: 2.4049\n",
            "Epoch [8/10], Step [170/400], Loss: 2.5139\n",
            "Epoch [8/10], Step [180/400], Loss: 2.6713\n",
            "Epoch [8/10], Step [190/400], Loss: 2.7251\n",
            "Epoch [8/10], Step [200/400], Loss: 2.3218\n",
            "Epoch [8/10], Step [210/400], Loss: 2.5058\n",
            "Epoch [8/10], Step [220/400], Loss: 2.7213\n",
            "Epoch [8/10], Step [230/400], Loss: 2.6127\n",
            "Epoch [8/10], Step [240/400], Loss: 2.6906\n",
            "Epoch [8/10], Step [250/400], Loss: 2.4816\n",
            "Epoch [8/10], Step [260/400], Loss: 2.5347\n",
            "Epoch [8/10], Step [270/400], Loss: 2.1484\n",
            "Epoch [8/10], Step [280/400], Loss: 2.5377\n",
            "Epoch [8/10], Step [290/400], Loss: 2.5120\n",
            "Epoch [8/10], Step [300/400], Loss: 2.5972\n",
            "Epoch [8/10], Step [310/400], Loss: 2.4188\n",
            "Epoch [8/10], Step [320/400], Loss: 2.4536\n",
            "Epoch [8/10], Step [330/400], Loss: 2.5804\n",
            "Epoch [8/10], Step [340/400], Loss: 2.6843\n",
            "Epoch [8/10], Step [350/400], Loss: 2.4848\n",
            "Epoch [8/10], Step [360/400], Loss: 2.9226\n",
            "Epoch [8/10], Step [370/400], Loss: 2.5074\n",
            "Epoch [8/10], Step [380/400], Loss: 2.6723\n",
            "Epoch [8/10], Step [390/400], Loss: 2.5179\n",
            "Epoch [8/10], Step [400/400], Loss: 2.5882\n",
            "Model for epoch 8 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4672\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Step [10/400], Loss: 2.4937\n",
            "Epoch [9/10], Step [20/400], Loss: 2.6033\n",
            "Epoch [9/10], Step [30/400], Loss: 2.5801\n",
            "Epoch [9/10], Step [40/400], Loss: 2.4726\n",
            "Epoch [9/10], Step [50/400], Loss: 2.5762\n",
            "Epoch [9/10], Step [60/400], Loss: 2.5412\n",
            "Epoch [9/10], Step [70/400], Loss: 2.5182\n",
            "Epoch [9/10], Step [80/400], Loss: 2.5373\n",
            "Epoch [9/10], Step [90/400], Loss: 2.5443\n",
            "Epoch [9/10], Step [100/400], Loss: 2.6840\n",
            "Epoch [9/10], Step [110/400], Loss: 2.5486\n",
            "Epoch [9/10], Step [120/400], Loss: 2.5106\n",
            "Epoch [9/10], Step [130/400], Loss: 2.8020\n",
            "Epoch [9/10], Step [140/400], Loss: 2.5160\n",
            "Epoch [9/10], Step [150/400], Loss: 2.6970\n",
            "Epoch [9/10], Step [160/400], Loss: 2.3613\n",
            "Epoch [9/10], Step [170/400], Loss: 2.5818\n",
            "Epoch [9/10], Step [180/400], Loss: 2.7712\n",
            "Epoch [9/10], Step [190/400], Loss: 2.4130\n",
            "Epoch [9/10], Step [200/400], Loss: 2.5778\n",
            "Epoch [9/10], Step [210/400], Loss: 2.4207\n",
            "Epoch [9/10], Step [220/400], Loss: 2.3272\n",
            "Epoch [9/10], Step [230/400], Loss: 2.2471\n",
            "Epoch [9/10], Step [240/400], Loss: 2.2651\n",
            "Epoch [9/10], Step [250/400], Loss: 2.5758\n",
            "Epoch [9/10], Step [260/400], Loss: 2.5457\n",
            "Epoch [9/10], Step [270/400], Loss: 2.5174\n",
            "Epoch [9/10], Step [280/400], Loss: 2.4568\n",
            "Epoch [9/10], Step [290/400], Loss: 2.2988\n",
            "Epoch [9/10], Step [300/400], Loss: 2.6763\n",
            "Epoch [9/10], Step [310/400], Loss: 2.3139\n",
            "Epoch [9/10], Step [320/400], Loss: 2.8878\n",
            "Epoch [9/10], Step [330/400], Loss: 2.6604\n",
            "Epoch [9/10], Step [340/400], Loss: 2.6869\n",
            "Epoch [9/10], Step [350/400], Loss: 2.6449\n",
            "Epoch [9/10], Step [360/400], Loss: 2.2412\n",
            "Epoch [9/10], Step [370/400], Loss: 2.7382\n",
            "Epoch [9/10], Step [380/400], Loss: 2.5496\n",
            "Epoch [9/10], Step [390/400], Loss: 2.3380\n",
            "Epoch [9/10], Step [400/400], Loss: 2.7576\n",
            "Model for epoch 9 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4586\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Step [10/400], Loss: 2.6091\n",
            "Epoch [10/10], Step [20/400], Loss: 2.4075\n",
            "Epoch [10/10], Step [30/400], Loss: 2.5876\n",
            "Epoch [10/10], Step [40/400], Loss: 2.5641\n",
            "Epoch [10/10], Step [50/400], Loss: 2.6242\n",
            "Epoch [10/10], Step [60/400], Loss: 2.5104\n",
            "Epoch [10/10], Step [70/400], Loss: 2.6237\n",
            "Epoch [10/10], Step [80/400], Loss: 2.7085\n",
            "Epoch [10/10], Step [90/400], Loss: 2.6933\n",
            "Epoch [10/10], Step [100/400], Loss: 2.6526\n",
            "Epoch [10/10], Step [110/400], Loss: 2.5021\n",
            "Epoch [10/10], Step [120/400], Loss: 2.4977\n",
            "Epoch [10/10], Step [130/400], Loss: 2.3335\n",
            "Epoch [10/10], Step [140/400], Loss: 2.5974\n",
            "Epoch [10/10], Step [150/400], Loss: 2.3683\n",
            "Epoch [10/10], Step [160/400], Loss: 2.3827\n",
            "Epoch [10/10], Step [170/400], Loss: 2.5508\n",
            "Epoch [10/10], Step [180/400], Loss: 2.3202\n",
            "Epoch [10/10], Step [190/400], Loss: 2.6397\n",
            "Epoch [10/10], Step [200/400], Loss: 2.5870\n",
            "Epoch [10/10], Step [210/400], Loss: 2.3348\n",
            "Epoch [10/10], Step [220/400], Loss: 2.6036\n",
            "Epoch [10/10], Step [230/400], Loss: 2.5952\n",
            "Epoch [10/10], Step [240/400], Loss: 2.5419\n",
            "Epoch [10/10], Step [250/400], Loss: 2.4740\n",
            "Epoch [10/10], Step [260/400], Loss: 2.3579\n",
            "Epoch [10/10], Step [270/400], Loss: 2.3995\n",
            "Epoch [10/10], Step [280/400], Loss: 2.5564\n",
            "Epoch [10/10], Step [290/400], Loss: 2.4883\n",
            "Epoch [10/10], Step [300/400], Loss: 2.5316\n",
            "Epoch [10/10], Step [310/400], Loss: 2.6151\n",
            "Epoch [10/10], Step [320/400], Loss: 2.4722\n",
            "Epoch [10/10], Step [330/400], Loss: 2.4825\n",
            "Epoch [10/10], Step [340/400], Loss: 2.5311\n",
            "Epoch [10/10], Step [350/400], Loss: 2.3563\n",
            "Epoch [10/10], Step [360/400], Loss: 2.7263\n",
            "Epoch [10/10], Step [370/400], Loss: 2.5306\n",
            "Epoch [10/10], Step [380/400], Loss: 2.6417\n",
            "Epoch [10/10], Step [390/400], Loss: 2.7731\n",
            "Epoch [10/10], Step [400/400], Loss: 2.6435\n",
            "Model for epoch 10 saved.\n",
            "================================================================================\n",
            "Training End..\n",
            "Validation Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 2.4647\n",
            "================================================================================\n",
            "Validation End..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example)**\n",
        "\n",
        "Batch outputs (raw): [3.7994716 3.7994716 3.7994716 3.7994716 3.7994716] \\\n",
        "Batch labels: [5. 3. 5. 5. 2.]"
      ],
      "metadata": {
        "id": "ld94tEv5I2UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Testing Loop =================\n",
        "print(\"=\" * 80)\n",
        "print(\"Testing Start...\")\n",
        "#all_outputs = []\n",
        "\n",
        "model.load_state_dict(torch.load(f\"{save_path}best_model.pkl\"))\n",
        "model.eval()                                                                        # Evaluation 모드로 설정\n",
        "\n",
        "test_outputs = []\n",
        "test_labels = []\n",
        "\n",
        "\n",
        "test_loss = 0\n",
        "num_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user_review, item_review, labels in test_loader:\n",
        "        user_review = to_var(user_review)\n",
        "        item_review = to_var(item_review)\n",
        "        labels = to_var(labels)\n",
        "\n",
        "######### 모델 예측\n",
        "        # Forward 예측\n",
        "        outputs = model(user_review, item_review)\n",
        "\n",
        "        test_outputs.extend(outputs.cpu().numpy())  # 예측값 저장\n",
        "        test_labels.extend(labels.cpu().numpy())    # 실제값 저장\n",
        "\n",
        "        # MSE 계산\n",
        "        test_loss += torch.sum((outputs - labels) ** 2).item()\n",
        "        num_samples += labels.size(0)\n",
        "\n",
        "# 결과 계산 + MSE 도출\n",
        "test_mse = test_loss / num_samples\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "\n",
        "\n",
        "# 결과 저장\n",
        "with open(\"/content/drive/MyDrive/IDS/amaxon reviews 2023/D-attn/result_test2.pickle\", \"wb\") as f:\n",
        "    pickle.dump(all_outputs, f)\n",
        "\n",
        "print(\"Testing End. Results saved to 'result_test2.pickle'.\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "9sXk7CasZJWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a93560-206c-4f12-f9d0-e59497a33630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Testing Start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-b39af9b1ea1c>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"{save_path}best_model.pkl\"))\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n",
            "<ipython-input-23-5ee29f52e361>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(user_review, dtype=torch.float), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 2.4165\n",
            "Testing End. Results saved to 'result_test2.pickle'.\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}
