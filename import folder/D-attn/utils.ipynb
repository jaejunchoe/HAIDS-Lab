{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxBo+qCcUW+VO6i8iahO2b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaejunchoe/HAIDS-Lab/blob/main/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug3vOTVdLjqD"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from nltk import word_tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "\n",
        "# Dataset Definition -> Role: Data load, Review preprocessing, Transforming embedding\n",
        "class ReviewDataset(Dataset):\n",
        "\n",
        "    # 데이터에 대한 리뷰 길이 10,000으로 제한하고, embed_size 기반으로 padding, unknown, delimiter(구분자) 데이터 설정하는 함수\n",
        "    def __init__(self, csv_path, emb_path, max_len=10000, embed_size=64):\n",
        "        \"\"\"\n",
        "        데이터셋 클래스\n",
        "        :param csv_path: 전처리된 CSV 파일 경로\n",
        "        :param emb_path: 단어 임베딩 파일 경로\n",
        "        :param max_len: 리뷰 최대 길이\n",
        "        :param embed_size: 임베딩 차원\n",
        "        \"\"\"\n",
        "        self.dataset = pd.read_csv(csv_path, header=None, names=['userID', 'itemID', 'review', 'rating'])\n",
        "\n",
        "        ## 단어 임베딩 로드\n",
        "        with open(emb_path, 'rb') as f:\n",
        "            self.word_emb = pickle.load(f)\n",
        "\n",
        "        # 임베딩 벡터 설정\n",
        "        self.pad = np.zeros(embed_size)\n",
        "        self.unknown = np.random.uniform(0, 1, embed_size)\n",
        "        self.delimiter = np.random.uniform(0, 1, embed_size)\n",
        "\n",
        "        # 하이퍼파라미터\n",
        "        self.max_len = max_len\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "\n",
        "    # user와 item review에 대해 preprocessing + preprocessing된 review와 rating을 반환하는 함수\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        하나의 데이터 샘플을 가져옵니다.\n",
        "        \"\"\"\n",
        "        row = self.dataset.loc[index]\n",
        "        user_id = row['userID']\n",
        "        item_id = row['itemID']\n",
        "\n",
        "        # 리뷰 데이터\n",
        "        user_review = self.preprocess_review(user_id, \"User\")\n",
        "        item_review = self.preprocess_review(item_id, \"Item\")\n",
        "\n",
        "        # 평점\n",
        "        rating = torch.tensor(row['rating'], dtype=torch.float)\n",
        "\n",
        "        return torch.tensor(user_review, dtype=torch.float), \\\n",
        "               torch.tensor(item_review, dtype=torch.float), \\\n",
        "               rating\n",
        "\n",
        "\n",
        "    # 데이터셋의 총 길이(샘플 개수)를 반환 -> 데이터셋의 크기 확인\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "\n",
        "    # user 또는 item id에 맞는 review를 읽고 토큰화한 뒤 word_emb에서 embedding vector로 변환하고 max_len에 맞추어 처리하는 함수\n",
        "    def preprocess_review(self, entity_id, entity_type):\n",
        "        \"\"\"\n",
        "        리뷰 파일을 로드하고 임베딩을 적용합니다.\n",
        "        \"\"\"\n",
        "        file_path = f\"data/{entity_type}/{entity_id}.tsv\"                   # entity_type: user or item에 대한 문자열로 user와 item을 구분\n",
        "                                                                            # entity_id: 해당 user의 id or item의 id\n",
        "                                                                            # 해당 부분들은 동적으로 생성되는 구조 = file_path\n",
        "\n",
        "\n",
        "\n",
        "        try:\n",
        "            reviews = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "        except Exception:\n",
        "            return [self.pad] * self.max_len                                # 만약 파일이 존재하지 않다면, [self.pad] * self.max_len로 반환.\n",
        "\n",
        "        total_review = []\n",
        "        for review_str in reviews[0][:100]:  # 최대 100개의 리뷰 단어\n",
        "            tokens = word_tokenize(review_str)\n",
        "            for word in tokens:\n",
        "                if word in self.word_emb:\n",
        "                    total_review.append(self.word_emb[word])                # word_emb에 있는 단어에는 embedding vector로 변환\n",
        "                else:\n",
        "                    total_review.append(self.unknown)                       # word_emb에서 없는 단어의 경우 unknown vector로 사용\n",
        "            total_review.append(self.delimiter)                             # 각 review 문장 끝날 때마다 구분 벡터로 리뷰 간의 경계선 역할을 진행\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ## 리뷰 길이 조정 (패딩 또는 자르기) -> max_len(10,000)보다 길면 자르고 짧으면 pad로 padding\n",
        "        if len(total_review) < self.max_len:\n",
        "            pad_len = self.max_len - len(total_review)\n",
        "            total_review += [self.pad] * pad_len\n",
        "        else:\n",
        "            total_review = total_review[:self.max_len]\n",
        "\n",
        "        return np.array(total_review)                                       # numpy 배열로 변환하여 반환하고 이 결과는 user or item의 review를 vector화한 max_len(10,000)의 길이 배열\n",
        "\n",
        "\n",
        "# Batch 생성 시 None 데이터를 제거 -> DataLoader에서 batch를 만들 때, None 값이 포함된 데이터는 제거하는 함수\n",
        "def my_collate(batch):\n",
        "    \"\"\"\n",
        "    None 데이터를 필터링하는 Collate 함수.\n",
        "    \"\"\"\n",
        "    batch = list(filter(lambda x: x is not None, batch))                    # batch = DataLoader로부터 전달된 데이터 리스트\n",
        "    return default_collate(batch)                                           # default_collate: pytorch에서 제공하는 배치 생성 함수로, tensor 형태로 변환\n",
        "\n",
        "\n",
        "\n",
        "# DataLoader를 생성하는 Pytorch 함수 -> ReviewDataset 클래스를 기반으로 batch 단위의 DataLoader를 반환하는 역할\n",
        "def get_loader(csv_path, emb_path, batch_size=32, shuffle=True, num_workers=2):\n",
        "    \"\"\"\n",
        "    데이터 로더 생성 함수.\n",
        "    :param csv_path: 전처리된 CSV 파일 경로\n",
        "    :param emb_path: 단어 임베딩 파일 경로\n",
        "    \"\"\"\n",
        "    dataset = ReviewDataset(csv_path, emb_path)                             # ReviewDataset 객체를 생성해 데이터셋을 초기화함.\n",
        "    data_loader = DataLoader(dataset=dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=my_collate)                         # collate_fn: 배치 생성 시 사용할 함수이며, def my_collate를 사용한다.\n",
        "    return data_loader\n"
      ]
    }
  ]
}
